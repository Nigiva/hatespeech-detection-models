{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RoBERTa BCE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables d'environnement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "# Id des GPU disponibles : 0 et 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import datetime\n",
    "from typing import Any, Union, Dict, List\n",
    "import uuid\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchtext\n",
    "import nltk\n",
    "import sklearn\n",
    "import transformers\n",
    "import torchmetrics as tm\n",
    "from torchmetrics import MetricCollection, Metric, Accuracy, Precision, Recall, AUROC, HammingDistance, F1, ROC, AUC, PrecisionRecallCurve\n",
    "\n",
    "\n",
    "from loguru import logger\n",
    "from tqdm.auto import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "# import warnings\n",
    "# warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUSTOME_NAME = \"roberta-bce\"\n",
    "\n",
    "# Dataset\n",
    "DATA_DIR_PATH = os.path.abspath(\"../../data\")\n",
    "TRAIN_DATASET_PATH = os.path.join(DATA_DIR_PATH, \"jigsaw2019-train.csv\")\n",
    "TEST_DATASET_PATH = os.path.join(DATA_DIR_PATH, \"jigsaw2019-test.csv\")\n",
    "LABEL_LIST = ['toxicity', 'severe_toxicity', 'obscene', 'sexual_explicit',\n",
    "            'identity_attack', 'insult', 'threat']\n",
    "IDENTITY_LIST = ['male', 'female', 'transgender', 'other_gender', 'heterosexual',\n",
    "                'homosexual_gay_or_lesbian', 'bisexual','other_sexual_orientation',\n",
    "                'christian', 'jewish', 'muslim', 'hindu','buddhist', 'atheist',\n",
    "                'other_religion', 'black', 'white', 'asian', 'latino',\n",
    "                'other_race_or_ethnicity', 'physical_disability',\n",
    "                'intellectual_or_learning_disability',\n",
    "                'psychiatric_or_mental_illness','other_disability']\n",
    "SELECTED_IDENTITY_LIST = ['male', 'female', 'black', 'white', 'homosexual_gay_or_lesbian',\n",
    "                    'christian', 'jewish', 'muslim', 'psychiatric_or_mental_illness']\n",
    "\n",
    "# Session\n",
    "SESSION_DIR_PATH = os.path.abspath(\"../../session\")\n",
    "SESSION_DATETIME = datetime.datetime.now().strftime(\"%Y-%m-%dT%H-%M-%S-%f\")\n",
    "SESSION_NAME = f\"{CUSTOME_NAME}_{SESSION_DATETIME}\"\n",
    "CURRENT_SESSION_DIR_PATH = os.path.join(SESSION_DIR_PATH, SESSION_NAME)\n",
    "# Créer le dossier de la session\n",
    "os.makedirs(CURRENT_SESSION_DIR_PATH, exist_ok=True)\n",
    "\n",
    "# Architecture de fichier dans `CURRENT_SESSION_DIR_PATH`\n",
    "LOG_FILE_NAME = f\"{SESSION_NAME}.loguru.log\"\n",
    "MODEL_FILE_NAME = f\"{SESSION_NAME}.model\"\n",
    "TEST_FILE_NAME = f\"{SESSION_NAME}.test.csv\"\n",
    "METRIC_FILE_NAME = f\"{SESSION_NAME}.metric.json\"\n",
    "LOG_FILE_PATH = os.path.join(CURRENT_SESSION_DIR_PATH, LOG_FILE_NAME)\n",
    "MODEL_FILE_PATH = os.path.join(CURRENT_SESSION_DIR_PATH, MODEL_FILE_NAME)\n",
    "TEST_FILE_PATH = os.path.join(CURRENT_SESSION_DIR_PATH, TEST_FILE_NAME)\n",
    "METRIC_FILE_PATH = os.path.join(CURRENT_SESSION_DIR_PATH, METRIC_FILE_NAME)\n",
    "\n",
    "# CUDA\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-25 00:11:39.739 | INFO     | __main__:<cell line: 2>:2 - SESSION_NAME='roberta-bce_2022-03-25T00-11-39-462950'\n",
      "2022-03-25 00:11:39.741 | INFO     | __main__:<cell line: 3>:3 - TRAIN_DATASET_PATH='/work2/home/ing1/corentin/hatespeech-detection-models/data/jigsaw2019-train.csv'\n",
      "2022-03-25 00:11:39.743 | INFO     | __main__:<cell line: 4>:4 - TEST_DATASET_PATH='/work2/home/ing1/corentin/hatespeech-detection-models/data/jigsaw2019-test.csv'\n",
      "2022-03-25 00:11:39.745 | INFO     | __main__:<cell line: 5>:5 - CURRENT_SESSION_DIR_PATH='/work2/home/ing1/corentin/hatespeech-detection-models/session/roberta-bce_2022-03-25T00-11-39-462950'\n"
     ]
    }
   ],
   "source": [
    "logger.add(LOG_FILE_PATH, level=\"TRACE\")\n",
    "logger.info(f\"{SESSION_NAME=}\")\n",
    "logger.info(f\"{TRAIN_DATASET_PATH=}\")\n",
    "logger.info(f\"{TEST_DATASET_PATH=}\")\n",
    "logger.info(f\"{CURRENT_SESSION_DIR_PATH=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vérifier la cohérence de l'architecture et l'accès aux ressources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-25 00:11:39.946 | INFO     | __main__:<cell line: 1>:1 - Checking consistency...\n",
      "2022-03-25 00:11:39.949 | SUCCESS  | __main__:<cell line: 10>:10 - Datasets are reachable\n",
      "2022-03-25 00:11:39.950 | INFO     | __main__:<cell line: 15>:15 - GPU_IS_AVAILABLE=True\n",
      "2022-03-25 00:11:39.951 | INFO     | __main__:<cell line: 16>:16 - GPU_COUNT=1\n",
      "2022-03-25 00:11:39.952 | SUCCESS  | __main__:<cell line: 20>:20 - GPU and CUDA are available\n",
      "2022-03-25 00:11:39.953 | INFO     | __main__:<cell line: 21>:21 - device=device(type='cuda')\n",
      "2022-03-25 00:11:39.954 | INFO     | __main__:<cell line: 22>:24 - GPU 0 : NVIDIA TITAN X (Pascal)\n"
     ]
    }
   ],
   "source": [
    "logger.info(f\"Checking consistency...\")\n",
    "\n",
    "# Vérifier l'accès aux datasets\n",
    "if not os.path.exists(TRAIN_DATASET_PATH):\n",
    "    logger.critical(f\"Train dataset does not exist !\")\n",
    "    raise RuntimeError(\"Train dataset does not exist !\")\n",
    "if not os.path.exists(TEST_DATASET_PATH):\n",
    "    logger.critical(f\"Test dataset does not exist !\")\n",
    "    raise RuntimeError(\"Test dataset does not exist !\")\n",
    "logger.success(\"Datasets are reachable\")\n",
    "\n",
    "# Vérifier l'accès aux GPU\n",
    "GPU_IS_AVAILABLE = torch.cuda.is_available()\n",
    "GPU_COUNT = torch.cuda.device_count()\n",
    "logger.info(f\"{GPU_IS_AVAILABLE=}\")\n",
    "logger.info(f\"{GPU_COUNT=}\")\n",
    "if not GPU_IS_AVAILABLE:\n",
    "    logger.critical(\"GPU and CUDA are not available !\")\n",
    "    raise RuntimeError(\"GPU and CUDA are not available !\")\n",
    "logger.success(\"GPU and CUDA are available\")\n",
    "logger.info(f\"{device=}\")\n",
    "for gpu_id in range(GPU_COUNT):\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    logger.info(f\"GPU {gpu_id} : {gpu_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-25 00:11:47.289 | SUCCESS  | __main__:<cell line: 2>:2 - Dataset loaded !\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv(TRAIN_DATASET_PATH, index_col=0)\n",
    "logger.success(\"Dataset loaded !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remplacer toutes les colonnes correspondantes aux labels par 1 ou 0\n",
    "# si la probabilité est supérieure ou égale à 0.5 ou non\n",
    "train_df[LABEL_LIST] = (train_df[LABEL_LIST]>=0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JigsawDataset(Dataset):\n",
    "    def __init__(self, data_df, tokenizer):\n",
    "        self.data = data_df\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        comment = self.data.iloc[index][\"comment_text\"]\n",
    "        label = torch.tensor(self.data.iloc[index][LABEL_LIST].tolist(), dtype=torch.float)\n",
    "        \n",
    "        token_list, attention_mask = self.text_to_token_and_mask(comment)\n",
    "\n",
    "        return dict(index=index, ids=token_list, mask=attention_mask, labels=label)\n",
    "    \n",
    "    def text_to_token_and_mask(self, input_text):\n",
    "        tokenization_dict = tokenizer.encode_plus(input_text,\n",
    "                                add_special_tokens=True,\n",
    "                                max_length=128,\n",
    "                                padding='max_length',\n",
    "                                truncation=True,\n",
    "                                return_attention_mask=True,\n",
    "                                return_tensors='pt')\n",
    "        token_list = tokenization_dict[\"input_ids\"].flatten()\n",
    "        attention_mask = tokenization_dict[\"attention_mask\"].flatten()\n",
    "        return (token_list, attention_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_lr(optim, lr):\n",
    "    '''\n",
    "    Set the learning rate in the optimizer\n",
    "    '''\n",
    "    for g in optim.param_groups:\n",
    "        g['lr'] = lr\n",
    "    return optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer class and functions for models and predictions\n",
    "\n",
    "class TransformerClassifierStack(nn.Module):\n",
    "    def __init__(self, tr_model, nb_labels, dropout_prob=0.4, freeze=False):\n",
    "        super().__init__()\n",
    "        self.tr_model = tr_model\n",
    "\n",
    "        # Stack features of 4 last encoders\n",
    "        self.hidden_dim = tr_model.config.hidden_size * 4\n",
    "\n",
    "        # hidden linear for the classification\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        self.hl = nn.Linear(self.hidden_dim, self.hidden_dim)\n",
    "\n",
    "        # Last Linear for the classification\n",
    "        self.last_l = nn.Linear(self.hidden_dim, nb_labels)\n",
    "\n",
    "        # freeze all the parameters if necessary\n",
    "        for param in self.tr_model.parameters():\n",
    "            param.requires_grad = not freeze\n",
    "\n",
    "        # init learning params of last layers\n",
    "        torch.nn.init.xavier_uniform_(self.hl.weight)\n",
    "        torch.nn.init.xavier_uniform_(self.last_l.weight)\n",
    "\n",
    "    def forward(self, ids, mask):\n",
    "        # ids = [batch_size, padded_seq_len]\n",
    "        # mask = [batch_size, padded_seq_len]\n",
    "        # mask: avoid to make self attention on padded data\n",
    "        tr_output = self.tr_model(input_ids=ids,\n",
    "                                  attention_mask=mask,\n",
    "                                  output_hidden_states=True)\n",
    "\n",
    "        # Get all the hidden states\n",
    "        hidden_states = tr_output['hidden_states']\n",
    "\n",
    "        # hs_* = [batch_size, padded_seq_len, 768]\n",
    "        hs_1 = hidden_states[-1][:, 0, :]\n",
    "        hs_2 = hidden_states[-2][:, 0, :]\n",
    "        hs_3 = hidden_states[-3][:, 0, :]\n",
    "        hs_4 = hidden_states[-4][:, 0, :]\n",
    "\n",
    "        # features_vec = [batch_size, 768 * 4]\n",
    "        features_vec = torch.cat([hs_1, hs_2, hs_3, hs_4], dim=-1)\n",
    "\n",
    "        x = self.dropout(features_vec)\n",
    "        x = self.hl(x)\n",
    "\n",
    "        # x = [batch_size, 768 * 4]\n",
    "        x = torch.tanh(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.last_l(x)\n",
    "\n",
    "        # x = [batch_size, 1]\n",
    "        return x\n",
    "\n",
    "def load_roberta_model(nb_labels):\n",
    "    '''\n",
    "    Load RoBERTa model without any checkpoint\n",
    "    RoBERTa for finetuning\n",
    "    '''\n",
    "    logger.info(f\"transformers.RobertaTokenizer : roberta-base\")\n",
    "    logger.info(f\"transformers.AutoModel : roberta-base\")\n",
    "    tokenizer = transformers.RobertaTokenizer.from_pretrained('roberta-base')\n",
    "    tr_model = transformers.AutoModel.from_pretrained('roberta-base')\n",
    "    model = TransformerClassifierStack(tr_model, nb_labels)\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "def load_roberta_pretrained(path, nb_labels, lr=2e-5):\n",
    "    '''\n",
    "    Load RoBERTa from checkout point (already trained on Hate Speech tasks)\n",
    "    '''\n",
    "    tokenizer = transformers.RobertaTokenizer.from_pretrained('roberta-base')\n",
    "    tr_model = transformers.AutoModel.from_pretrained('roberta-base')\n",
    "    model = TransformerClassifierStack(tr_model, nb_labels)\n",
    "\n",
    "    loaded = torch.load(path)\n",
    "    model.load_state_dict(loaded['state_dict'])\n",
    "\n",
    "    optimizer = transformers.AdamW(model.parameters(), lr=lr)\n",
    "    optimizer.load_state_dict(loaded['optimizer_dict'])\n",
    "    optimizer = set_lr(optimizer, lr)\n",
    "\n",
    "    return model, tokenizer, optimizer\n",
    "\n",
    "def preds_fn(batch, model, device):\n",
    "    '''\n",
    "    Get the predictions for one batch according to the model\n",
    "    '''\n",
    "    b_input = batch['ids'].to(device)\n",
    "    b_mask = batch['mask'].to(device)\n",
    "\n",
    "    return model(b_input, b_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-25 00:11:49.285 | INFO     | __main__:load_roberta_model:62 - transformers.RobertaTokenizer : roberta-base\n",
      "2022-03-25 00:11:49.288 | INFO     | __main__:load_roberta_model:63 - transformers.AutoModel : roberta-base\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "2022-03-25 00:11:53.496 | SUCCESS  | __main__:<cell line: 3>:3 - Model loaded !\n"
     ]
    }
   ],
   "source": [
    "# Load the model\n",
    "model, tokenizer = load_roberta_model(nb_labels=len(LABEL_LIST))\n",
    "logger.success(\"Model loaded !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparamètre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-25 00:11:53.816 | INFO     | __main__:<cell line: 7>:7 - BATCH_SIZE=32\n",
      "2022-03-25 00:11:53.819 | INFO     | __main__:<cell line: 8>:8 - LR=0.0001\n",
      "2022-03-25 00:11:53.820 | INFO     | __main__:<cell line: 9>:9 - PIN_MEMORY=True\n",
      "2022-03-25 00:11:53.822 | INFO     | __main__:<cell line: 10>:10 - NUM_WORKERS=0\n",
      "2022-03-25 00:11:53.823 | INFO     | __main__:<cell line: 11>:11 - PREFETCH_FACTOR=2\n",
      "2022-03-25 00:11:53.824 | INFO     | __main__:<cell line: 12>:12 - NUM_EPOCHS=1\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 32\n",
    "LR=1e-4\n",
    "PIN_MEMORY = True\n",
    "NUM_WORKERS = 0\n",
    "PREFETCH_FACTOR = 2\n",
    "NUM_EPOCHS = 1\n",
    "logger.info(f\"{BATCH_SIZE=}\")\n",
    "logger.info(f\"{LR=}\")\n",
    "logger.info(f\"{PIN_MEMORY=}\")\n",
    "logger.info(f\"{NUM_WORKERS=}\")\n",
    "logger.info(f\"{PREFETCH_FACTOR=}\")\n",
    "logger.info(f\"{NUM_EPOCHS=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-25 00:18:53.409 | INFO     | __main__:<cell line: 23>:23 - BCEWithLogitsLoss()\n",
      "2022-03-25 00:18:53.411 | INFO     | __main__:<cell line: 25>:25 - AdamW (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.0001\n",
      "    maximize: False\n",
      "    weight_decay: 0.01\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BCEWithLogitsLoss()"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = JigsawDataset(train_df, tokenizer)\n",
    "train_dataloader = DataLoader(train_dataset,\n",
    "                             batch_size=BATCH_SIZE,\n",
    "                             shuffle=True,\n",
    "                             num_workers=NUM_WORKERS,\n",
    "                             prefetch_factor=PREFETCH_FACTOR,\n",
    "                             pin_memory=PIN_MEMORY)\n",
    "\n",
    "# Pseudo validation car c'est un sous-ensemble du jeu d'entrainement\n",
    "# C'est juste pour savoir si l'entraînement s'est bien passé\n",
    "# Il ne faut pas s'en servir pour optimiser les hyperparamètres\n",
    "validation_df = train_df.sample(n=50_000)\n",
    "validation_dataset = JigsawDataset(validation_df, tokenizer)\n",
    "validation_dataloader = DataLoader(validation_dataset,\n",
    "                             batch_size=BATCH_SIZE,\n",
    "                             shuffle=True,\n",
    "                             num_workers=NUM_WORKERS,\n",
    "                             prefetch_factor=PREFETCH_FACTOR,\n",
    "                             pin_memory=PIN_MEMORY)\n",
    "\n",
    "# Pas besoin de Sigmoid en sorti du model seulement pour `BCEWithLogitsLoss`\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "logger.info(criterion)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR)\n",
    "logger.info(optimizer)\n",
    "\n",
    "model.to(device)\n",
    "criterion.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variantes de Hamming Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HammingLossWithoutThreshold(Metric):\n",
    "    def __init__(self, num_classes=1, dist_sync_on_step=False):\n",
    "        super().__init__(dist_sync_on_step=dist_sync_on_step)\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        self.add_state(\"total\", default=torch.tensor(0), dist_reduce_fx=\"sum\")\n",
    "        self.add_state(\"nbr_sample\", default=torch.tensor(0), dist_reduce_fx=\"sum\")\n",
    "\n",
    "    def update(self, preds: torch.Tensor, target: torch.Tensor):\n",
    "        current_nbr_sample, current_nbr_category = preds.shape\n",
    "        if current_nbr_category != self.num_classes:\n",
    "          raise AttributeError(\"`num_classes` != `current_nbr_category` detected in `pred` parameter\")\n",
    "        \n",
    "        current_loss_per_pred = torch.absolute(target - preds)\n",
    "        current_hamming_loss = current_loss_per_pred.sum()\n",
    "\n",
    "        self.total += current_hamming_loss.float()\n",
    "        self.nbr_sample += current_nbr_sample\n",
    "\n",
    "    def compute(self):\n",
    "        return self.total/(self.num_classes*self.nbr_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RebalancedHammingLossWithoutThreshold(Metric):\n",
    "    def __init__(self, num_classes=1, average=\"macro\", dist_sync_on_step=False):\n",
    "        super().__init__(dist_sync_on_step=dist_sync_on_step)\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        # average = \"macro\" or None\n",
    "        self.average = average\n",
    "\n",
    "        # Nombre de positif 1 & negatif 0 par categorie\n",
    "        self.add_state(\n",
    "            \"number_positive\",\n",
    "            default=torch.tensor([0 for _ in range(num_classes)]),\n",
    "            dist_reduce_fx=\"sum\",\n",
    "        )\n",
    "        self.add_state(\n",
    "            \"number_negative\",\n",
    "            default=torch.tensor([0 for _ in range(num_classes)]),\n",
    "            dist_reduce_fx=\"sum\",\n",
    "        )\n",
    "\n",
    "        self.add_state(\n",
    "            \"hamming_loss_positive\",\n",
    "            default=torch.tensor([0.0 for _ in range(num_classes)]),\n",
    "            dist_reduce_fx=\"sum\",\n",
    "        )\n",
    "        self.add_state(\n",
    "            \"hamming_loss_negative\",\n",
    "            default=torch.tensor([0.0 for _ in range(num_classes)]),\n",
    "            dist_reduce_fx=\"sum\",\n",
    "        )\n",
    "\n",
    "        self.add_state(\"nbr_sample\", default=torch.tensor(0), dist_reduce_fx=\"sum\")\n",
    "\n",
    "    def update(self, preds: torch.Tensor, target: torch.Tensor):\n",
    "        current_nbr_sample, current_nbr_category = preds.shape\n",
    "        if current_nbr_category != self.num_classes:\n",
    "            raise AttributeError(\n",
    "                \"`num_classes` != `current_nbr_category` detected in `pred` parameter\"\n",
    "            )\n",
    "\n",
    "        # Nombre de positif 1 & negatif 0 par categorie\n",
    "        current_number_positive = target.sum(axis=0)\n",
    "        current_number_negative = current_nbr_sample - target.sum(axis=0)\n",
    "\n",
    "        self.number_positive += current_number_positive.int()\n",
    "        self.number_negative += current_number_negative.int()\n",
    "\n",
    "        self.nbr_sample += current_nbr_sample\n",
    "\n",
    "        for class_id in range(self.num_classes):\n",
    "            positive_filter = target[:, class_id] == 1\n",
    "            negative_filter = target[:, class_id] == 0\n",
    "\n",
    "            target_vector = target[:, class_id]\n",
    "            preds_vector = preds[:, class_id]\n",
    "\n",
    "            # Filtered vector\n",
    "            ## Target\n",
    "            pos_filtered_target_vector = target_vector[positive_filter]\n",
    "            neg_filtered_target_vector = target_vector[negative_filter]\n",
    "            ## Preds\n",
    "            pos_filtered_preds_vector = preds_vector[positive_filter]\n",
    "            neg_filtered_preds_vector = preds_vector[negative_filter]\n",
    "\n",
    "            # Hamming Loss without Threshold\n",
    "            hamming_loss_on_positive = torch.absolute(\n",
    "                pos_filtered_target_vector - pos_filtered_preds_vector\n",
    "            )\n",
    "            hamming_loss_on_negative = torch.absolute(\n",
    "                neg_filtered_target_vector - neg_filtered_preds_vector\n",
    "            )\n",
    "\n",
    "            self.hamming_loss_positive[class_id] += hamming_loss_on_positive.sum()\n",
    "            self.hamming_loss_negative[class_id] += hamming_loss_on_negative.sum()\n",
    "\n",
    "    def compute(self):\n",
    "        factor_pos = self.nbr_sample / (2 * self.number_positive)\n",
    "        factor_neg = self.nbr_sample / (2 * self.number_negative)\n",
    "\n",
    "        rebalanced_hamming_loss_per_class = torch.multiply(\n",
    "            self.hamming_loss_positive, factor_pos\n",
    "        ) + torch.multiply(self.hamming_loss_negative, factor_neg)\n",
    "        if self.average == \"macro\":\n",
    "            return rebalanced_hamming_loss_per_class.sum() / (\n",
    "                self.nbr_sample * self.num_classes\n",
    "            )\n",
    "        return rebalanced_hamming_loss_per_class / (self.nbr_sample)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instanciation des metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: Metric `AUROC` will save all targets and predictions in buffer. For large datasets this may lead to large memory footprint.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "num_classes = len(LABEL_LIST)\n",
    "train_metric_dict = dict()\n",
    "\n",
    "# AUROC Macro\n",
    "auroc_macro = AUROC(num_classes=num_classes, compute_on_step=True, average=\"macro\")\n",
    "train_metric_dict[\"auroc_macro\"] = auroc_macro\n",
    "\n",
    "# AUROC per class\n",
    "auroc_per_class = AUROC(num_classes=num_classes, compute_on_step=True, average=None)\n",
    "train_metric_dict[\"auroc_per_class\"] = auroc_per_class\n",
    "\n",
    "# Hamming Distance without Threshold\n",
    "hamming_loss_woutt = HammingLossWithoutThreshold(num_classes=num_classes)\n",
    "train_metric_dict[\"hamming_loss_without_threshold\"] = hamming_loss_woutt\n",
    "\n",
    "# Rebalanced Hamming Distance without Threshold macro\n",
    "rebalanced_hamming_loss_woutt_macro = RebalancedHammingLossWithoutThreshold(\n",
    "    num_classes=num_classes, average=\"macro\"\n",
    ")\n",
    "train_metric_dict[\n",
    "    \"rebalanced_hamming_loss_without_threshold_macro\"\n",
    "] = rebalanced_hamming_loss_woutt_macro\n",
    "\n",
    "# Rebalanced Hamming Distance without Threshold macro\n",
    "rebalanced_hamming_loss_woutt_per_class = RebalancedHammingLossWithoutThreshold(\n",
    "    num_classes=num_classes, average=None\n",
    ")\n",
    "train_metric_dict[\n",
    "    \"rebalanced_hamming_loss_without_threshold_per_class\"\n",
    "] = rebalanced_hamming_loss_woutt_per_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MetricCollection(\n",
       "  (auroc_macro): AUROC()\n",
       "  (auroc_per_class): AUROC()\n",
       "  (hamming_loss_without_threshold): HammingLossWithoutThreshold()\n",
       "  (rebalanced_hamming_loss_without_threshold_macro): RebalancedHammingLossWithoutThreshold()\n",
       "  (rebalanced_hamming_loss_without_threshold_per_class): RebalancedHammingLossWithoutThreshold()\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_metric = MetricCollection(train_metric_dict)\n",
    "train_metric.to(device)\n",
    "\n",
    "validation_metric = train_metric.clone()\n",
    "validation_metric.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def serialize(object_to_serialize: Any, ensure_ascii: bool = True) -> str:\n",
    "    \"\"\"\n",
    "    Serialize any object, i.e. convert an object to JSON\n",
    "    Args:\n",
    "        object_to_serialize (Any): The object to serialize\n",
    "        ensure_ascii (bool, optional): If ensure_ascii is true (the default), the output is guaranteed to have all incoming non-ASCII characters escaped. If ensure_ascii is false, these characters will be output as-is. Defaults to True.\n",
    "    Returns:\n",
    "            str: string of serialized object (JSON)\n",
    "    \"\"\"\n",
    "\n",
    "    def dumper(obj: Any) -> Union[str, Dict]:\n",
    "        \"\"\"\n",
    "        Function called recursively by json.dumps to know how to serialize an object.\n",
    "        For example, for datetime, we try to convert it to ISO format rather than\n",
    "        retrieve the list of attributes defined in its object.\n",
    "        Args:\n",
    "            obj (Any): The object to serialize\n",
    "        Returns:\n",
    "            Union[str, Dict]: Serialized object\n",
    "        \"\"\"\n",
    "        if isinstance(obj, torch.Tensor):\n",
    "            return obj.cpu().numpy().tolist()\n",
    "        elif hasattr(obj, \"__dict__\"):\n",
    "            return obj.__dict__\n",
    "        return str(obj)\n",
    "\n",
    "    return json.dumps(object_to_serialize, default=dumper, ensure_ascii=ensure_ascii)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_metric(metric_collection, **kwargs):\n",
    "    \"\"\"\n",
    "    Export MetricCollection to json file\n",
    "\n",
    "    Args:\n",
    "        metric_collection: MetricCollection\n",
    "        **kwargs: field to add in json line\n",
    "    \"\"\"\n",
    "    with open(METRIC_FILE_PATH, \"a\") as f:\n",
    "        metric_collection_value = metric_collection.compute()\n",
    "        metric_collection_value.update(kwargs)\n",
    "        serialized_value = serialize(metric_collection_value)\n",
    "        f.write(serialized_value)\n",
    "        f.write(\"\\n\")\n",
    "    logger.success(\"Metrics are exported !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entraînement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input nan is not valid. Should be a string, a list/tuple of strings or a list/tuple of integers.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/work2/home/ing1/corentin/hatespeech-detection-models/src/model/roberta-bce.ipynb Cell 34'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B91.243.117.249/work2/home/ing1/corentin/hatespeech-detection-models/src/model/roberta-bce.ipynb#ch0000088vscode-remote?line=0'>1</a>\u001b[0m \u001b[39mfor\u001b[39;00m batch_id, batch \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(train_dataloader):\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B91.243.117.249/work2/home/ing1/corentin/hatespeech-detection-models/src/model/roberta-bce.ipynb#ch0000088vscode-remote?line=1'>2</a>\u001b[0m     logger\u001b[39m.\u001b[39mtrace(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mbatch_id\u001b[39m=}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B91.243.117.249/work2/home/ing1/corentin/hatespeech-detection-models/src/model/roberta-bce.ipynb#ch0000088vscode-remote?line=2'>3</a>\u001b[0m     token_list_batch \u001b[39m=\u001b[39m batch[\u001b[39m\"\u001b[39m\u001b[39mids\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/torch/utils/data/dataloader.py:530\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/torch/utils/data/dataloader.py?line=527'>528</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/torch/utils/data/dataloader.py?line=528'>529</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()\n\u001b[0;32m--> <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/torch/utils/data/dataloader.py?line=529'>530</a>\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/torch/utils/data/dataloader.py?line=530'>531</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/torch/utils/data/dataloader.py?line=531'>532</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/torch/utils/data/dataloader.py?line=532'>533</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/torch/utils/data/dataloader.py?line=533'>534</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/torch/utils/data/dataloader.py:570\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/torch/utils/data/dataloader.py?line=567'>568</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/torch/utils/data/dataloader.py?line=568'>569</a>\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/torch/utils/data/dataloader.py?line=569'>570</a>\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/torch/utils/data/dataloader.py?line=570'>571</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/torch/utils/data/dataloader.py?line=571'>572</a>\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data)\n",
      "File \u001b[0;32m~/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py?line=46'>47</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfetch\u001b[39m(\u001b[39mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py?line=47'>48</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[0;32m---> <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py?line=48'>49</a>\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py?line=49'>50</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py?line=50'>51</a>\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py?line=46'>47</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfetch\u001b[39m(\u001b[39mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py?line=47'>48</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[0;32m---> <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py?line=48'>49</a>\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py?line=49'>50</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py?line=50'>51</a>\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "\u001b[1;32m/work2/home/ing1/corentin/hatespeech-detection-models/src/model/roberta-bce.ipynb Cell 15'\u001b[0m in \u001b[0;36mJigsawDataset.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B91.243.117.249/work2/home/ing1/corentin/hatespeech-detection-models/src/model/roberta-bce.ipynb#ch0000035vscode-remote?line=9'>10</a>\u001b[0m comment \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39miloc[index][\u001b[39m\"\u001b[39m\u001b[39mcomment_text\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B91.243.117.249/work2/home/ing1/corentin/hatespeech-detection-models/src/model/roberta-bce.ipynb#ch0000035vscode-remote?line=10'>11</a>\u001b[0m label \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39miloc[index][LABEL_LIST]\u001b[39m.\u001b[39mtolist(), dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mfloat)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B91.243.117.249/work2/home/ing1/corentin/hatespeech-detection-models/src/model/roberta-bce.ipynb#ch0000035vscode-remote?line=12'>13</a>\u001b[0m token_list, attention_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtext_to_token_and_mask(comment)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B91.243.117.249/work2/home/ing1/corentin/hatespeech-detection-models/src/model/roberta-bce.ipynb#ch0000035vscode-remote?line=14'>15</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mdict\u001b[39m(index\u001b[39m=\u001b[39mindex, ids\u001b[39m=\u001b[39mtoken_list, mask\u001b[39m=\u001b[39mattention_mask, labels\u001b[39m=\u001b[39mlabel)\n",
      "\u001b[1;32m/work2/home/ing1/corentin/hatespeech-detection-models/src/model/roberta-bce.ipynb Cell 15'\u001b[0m in \u001b[0;36mJigsawDataset.text_to_token_and_mask\u001b[0;34m(self, input_text)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B91.243.117.249/work2/home/ing1/corentin/hatespeech-detection-models/src/model/roberta-bce.ipynb#ch0000035vscode-remote?line=16'>17</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtext_to_token_and_mask\u001b[39m(\u001b[39mself\u001b[39m, input_text):\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B91.243.117.249/work2/home/ing1/corentin/hatespeech-detection-models/src/model/roberta-bce.ipynb#ch0000035vscode-remote?line=17'>18</a>\u001b[0m     tokenization_dict \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39;49mencode_plus(input_text,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B91.243.117.249/work2/home/ing1/corentin/hatespeech-detection-models/src/model/roberta-bce.ipynb#ch0000035vscode-remote?line=18'>19</a>\u001b[0m                             add_special_tokens\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B91.243.117.249/work2/home/ing1/corentin/hatespeech-detection-models/src/model/roberta-bce.ipynb#ch0000035vscode-remote?line=19'>20</a>\u001b[0m                             max_length\u001b[39m=\u001b[39;49m\u001b[39m128\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B91.243.117.249/work2/home/ing1/corentin/hatespeech-detection-models/src/model/roberta-bce.ipynb#ch0000035vscode-remote?line=20'>21</a>\u001b[0m                             padding\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mmax_length\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B91.243.117.249/work2/home/ing1/corentin/hatespeech-detection-models/src/model/roberta-bce.ipynb#ch0000035vscode-remote?line=21'>22</a>\u001b[0m                             truncation\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B91.243.117.249/work2/home/ing1/corentin/hatespeech-detection-models/src/model/roberta-bce.ipynb#ch0000035vscode-remote?line=22'>23</a>\u001b[0m                             return_attention_mask\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B91.243.117.249/work2/home/ing1/corentin/hatespeech-detection-models/src/model/roberta-bce.ipynb#ch0000035vscode-remote?line=23'>24</a>\u001b[0m                             return_tensors\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mpt\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B91.243.117.249/work2/home/ing1/corentin/hatespeech-detection-models/src/model/roberta-bce.ipynb#ch0000035vscode-remote?line=24'>25</a>\u001b[0m     token_list \u001b[39m=\u001b[39m tokenization_dict[\u001b[39m\"\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mflatten()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B91.243.117.249/work2/home/ing1/corentin/hatespeech-detection-models/src/model/roberta-bce.ipynb#ch0000035vscode-remote?line=25'>26</a>\u001b[0m     attention_mask \u001b[39m=\u001b[39m tokenization_dict[\u001b[39m\"\u001b[39m\u001b[39mattention_mask\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mflatten()\n",
      "File \u001b[0;32m~/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2556\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2545'>2546</a>\u001b[0m \u001b[39m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[1;32m   <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2546'>2547</a>\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[1;32m   <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2547'>2548</a>\u001b[0m     padding\u001b[39m=\u001b[39mpadding,\n\u001b[1;32m   <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2548'>2549</a>\u001b[0m     truncation\u001b[39m=\u001b[39mtruncation,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2552'>2553</a>\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m   <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2553'>2554</a>\u001b[0m )\n\u001b[0;32m-> <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2555'>2556</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_encode_plus(\n\u001b[1;32m   <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2556'>2557</a>\u001b[0m     text\u001b[39m=\u001b[39;49mtext,\n\u001b[1;32m   <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2557'>2558</a>\u001b[0m     text_pair\u001b[39m=\u001b[39;49mtext_pair,\n\u001b[1;32m   <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2558'>2559</a>\u001b[0m     add_special_tokens\u001b[39m=\u001b[39;49madd_special_tokens,\n\u001b[1;32m   <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2559'>2560</a>\u001b[0m     padding_strategy\u001b[39m=\u001b[39;49mpadding_strategy,\n\u001b[1;32m   <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2560'>2561</a>\u001b[0m     truncation_strategy\u001b[39m=\u001b[39;49mtruncation_strategy,\n\u001b[1;32m   <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2561'>2562</a>\u001b[0m     max_length\u001b[39m=\u001b[39;49mmax_length,\n\u001b[1;32m   <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2562'>2563</a>\u001b[0m     stride\u001b[39m=\u001b[39;49mstride,\n\u001b[1;32m   <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2563'>2564</a>\u001b[0m     is_split_into_words\u001b[39m=\u001b[39;49mis_split_into_words,\n\u001b[1;32m   <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2564'>2565</a>\u001b[0m     pad_to_multiple_of\u001b[39m=\u001b[39;49mpad_to_multiple_of,\n\u001b[1;32m   <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2565'>2566</a>\u001b[0m     return_tensors\u001b[39m=\u001b[39;49mreturn_tensors,\n\u001b[1;32m   <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2566'>2567</a>\u001b[0m     return_token_type_ids\u001b[39m=\u001b[39;49mreturn_token_type_ids,\n\u001b[1;32m   <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2567'>2568</a>\u001b[0m     return_attention_mask\u001b[39m=\u001b[39;49mreturn_attention_mask,\n\u001b[1;32m   <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2568'>2569</a>\u001b[0m     return_overflowing_tokens\u001b[39m=\u001b[39;49mreturn_overflowing_tokens,\n\u001b[1;32m   <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2569'>2570</a>\u001b[0m     return_special_tokens_mask\u001b[39m=\u001b[39;49mreturn_special_tokens_mask,\n\u001b[1;32m   <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2570'>2571</a>\u001b[0m     return_offsets_mapping\u001b[39m=\u001b[39;49mreturn_offsets_mapping,\n\u001b[1;32m   <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2571'>2572</a>\u001b[0m     return_length\u001b[39m=\u001b[39;49mreturn_length,\n\u001b[1;32m   <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2572'>2573</a>\u001b[0m     verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m   <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2573'>2574</a>\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m   <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2574'>2575</a>\u001b[0m )\n",
      "File \u001b[0;32m~/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/transformers/tokenization_utils.py:647\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/transformers/tokenization_utils.py?line=637'>638</a>\u001b[0m \u001b[39mif\u001b[39;00m return_offsets_mapping:\n\u001b[1;32m    <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/transformers/tokenization_utils.py?line=638'>639</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m(\n\u001b[1;32m    <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/transformers/tokenization_utils.py?line=639'>640</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mreturn_offset_mapping is not available when using Python tokenizers. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/transformers/tokenization_utils.py?line=640'>641</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mTo use this feature, change your tokenizer to one deriving from \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/transformers/tokenization_utils.py?line=643'>644</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mhttps://github.com/huggingface/transformers/pull/2674\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/transformers/tokenization_utils.py?line=644'>645</a>\u001b[0m     )\n\u001b[0;32m--> <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/transformers/tokenization_utils.py?line=646'>647</a>\u001b[0m first_ids \u001b[39m=\u001b[39m get_input_ids(text)\n\u001b[1;32m    <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/transformers/tokenization_utils.py?line=647'>648</a>\u001b[0m second_ids \u001b[39m=\u001b[39m get_input_ids(text_pair) \u001b[39mif\u001b[39;00m text_pair \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/transformers/tokenization_utils.py?line=649'>650</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprepare_for_model(\n\u001b[1;32m    <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/transformers/tokenization_utils.py?line=650'>651</a>\u001b[0m     first_ids,\n\u001b[1;32m    <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/transformers/tokenization_utils.py?line=651'>652</a>\u001b[0m     pair_ids\u001b[39m=\u001b[39msecond_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/transformers/tokenization_utils.py?line=665'>666</a>\u001b[0m     verbose\u001b[39m=\u001b[39mverbose,\n\u001b[1;32m    <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/transformers/tokenization_utils.py?line=666'>667</a>\u001b[0m )\n",
      "File \u001b[0;32m~/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/transformers/tokenization_utils.py:634\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._encode_plus.<locals>.get_input_ids\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m    <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/transformers/tokenization_utils.py?line=629'>630</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/transformers/tokenization_utils.py?line=630'>631</a>\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mInput \u001b[39m\u001b[39m{\u001b[39;00mtext\u001b[39m}\u001b[39;00m\u001b[39m is not valid. Should be a string or a list/tuple of strings when `is_split_into_words=True`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/transformers/tokenization_utils.py?line=631'>632</a>\u001b[0m     )\n\u001b[1;32m    <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/transformers/tokenization_utils.py?line=632'>633</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/transformers/tokenization_utils.py?line=633'>634</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/transformers/tokenization_utils.py?line=634'>635</a>\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mInput \u001b[39m\u001b[39m{\u001b[39;00mtext\u001b[39m}\u001b[39;00m\u001b[39m is not valid. Should be a string, a list/tuple of strings or a list/tuple of integers.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/transformers/tokenization_utils.py?line=635'>636</a>\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Input nan is not valid. Should be a string, a list/tuple of strings or a list/tuple of integers."
     ]
    }
   ],
   "source": [
    "for batch_id, batch in enumerate(train_dataloader):\n",
    "    logger.trace(f\"{batch_id=}\")\n",
    "    token_list_batch = batch[\"ids\"].to(device)\n",
    "    attention_mask_batch = batch[\"mask\"].to(device)\n",
    "    label_batch = batch[\"labels\"].to(device)\n",
    "    prediction_batch = model(token_list_batch, attention_mask_batch)\n",
    "    transformed_prediction_batch = prediction_batch.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(batch[\"ids\"])):\n",
    "    eetoken_list_batch = batch[\"ids\"][i:i+1].to(device)\n",
    "    eeattention_mask_batch = batch[\"mask\"][i:i+1].to(device)\n",
    "    eelabel_batch = batch[\"labels\"][i:i+1].to(device)\n",
    "    eeprediction_batch = model(eetoken_list_batch, eeattention_mask_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(epoch_id=None):\n",
    "    model.train()\n",
    "    logger.info(f\"START EPOCH {epoch_id=}\")\n",
    "\n",
    "    progress = tqdm(train_dataloader, desc='training batch...', leave=False)\n",
    "    for batch_id, batch in enumerate(progress):\n",
    "        logger.trace(f\"{batch_id=}\")\n",
    "        token_list_batch = batch[\"ids\"].to(device)\n",
    "        attention_mask_batch = batch[\"mask\"].to(device)\n",
    "        label_batch = batch[\"labels\"].to(device)\n",
    "\n",
    "        # Predict\n",
    "        prediction_batch = model(token_list_batch, attention_mask_batch)\n",
    "        transformed_prediction_batch = prediction_batch.squeeze()\n",
    "\n",
    "        # Loss\n",
    "        loss = criterion(transformed_prediction_batch.to(torch.float32), label_batch.to(torch.float32))\n",
    "\n",
    "        # Metrics\n",
    "        train_metrics_collection_dict = train_metric(transformed_prediction_batch.to(torch.float32), label_batch.to(torch.int32))\n",
    "        logger.trace(train_metrics_collection_dict)\n",
    "\n",
    "        # Backprop\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update progress bar description\n",
    "        progress_description = \"Train Loss : {loss:.4f} - Train AUROC : {acc:.4f}\"\n",
    "        auroc_macro_value = float(train_metrics_collection_dict[\"auroc_macro\"])\n",
    "        progress_description = progress_description.format(loss=loss.item(), acc=auroc_macro_value)\n",
    "        progress.set_description(progress_description)\n",
    "\n",
    "    logger.info(f\"END EPOCH {epoch_id=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def valid_epoch(epoch_id=None):\n",
    "    model.eval()\n",
    "    logger.info(f\"START VALIDATION {epoch_id=}\")\n",
    "    validation_metric.reset()\n",
    "\n",
    "    loss_list = []\n",
    "    prediction_list = torch.Tensor([])\n",
    "    target_list = torch.Tensor([])\n",
    "\n",
    "\n",
    "    progress = tqdm(validation_dataloader, desc=\"valid batch...\", leave=False)\n",
    "    for batch_id, batch in enumerate(progress):\n",
    "        logger.trace(f\"{batch_id=}\")\n",
    "        \n",
    "        token_list_batch = batch[\"ids\"].to(device)\n",
    "        attention_mask_batch = batch[\"mask\"].to(device)\n",
    "        label_batch = batch[\"labels\"].to(device)\n",
    "\n",
    "        # Predict\n",
    "        prediction_batch = model(token_list_batch, attention_mask_batch)\n",
    "\n",
    "        transformed_prediction_batch = prediction_batch.squeeze()\n",
    "\n",
    "        # Loss\n",
    "        loss = criterion(\n",
    "            transformed_prediction_batch.to(torch.float32),\n",
    "            label_batch.to(torch.float32),\n",
    "        )\n",
    "        loss_list.append(loss.item())\n",
    "        prediction_list = torch.concat(\n",
    "            [prediction_list, transformed_prediction_batch.cpu()]\n",
    "        )\n",
    "        target_list = torch.concat([target_list, label_batch.cpu()])\n",
    "\n",
    "        # Metrics\n",
    "        validation_metric(transformed_prediction_batch.to(torch.float32), label_batch.to(torch.int32))\n",
    "\n",
    "    loss_mean = np.mean(loss_list)\n",
    "    logger.trace(validation_metric.compute())\n",
    "    logger.info(f\"END VALIDATION {epoch_id=}\")\n",
    "    export_metric(validation_metric, epoch_id=epoch_id, loss=loss_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93b999291eda4a6ea80fb6d37fac78f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training epoch...:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-25 00:12:00.318 | INFO     | __main__:train_epoch:3 - START EPOCH epoch_id=1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6520eddb0f7f4e668d0bf8dad446b6aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training batch...:   0%|          | 0/56403 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "Input nan is not valid. Should be a string, a list/tuple of strings or a list/tuple of integers.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/work2/home/ing1/corentin/hatespeech-detection-models/src/model/roberta-bce.ipynb Cell 36'\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B91.243.117.249/work2/home/ing1/corentin/hatespeech-detection-models/src/model/roberta-bce.ipynb#ch0000070vscode-remote?line=1'>2</a>\u001b[0m progress \u001b[39m=\u001b[39m  tqdm(\u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m,NUM_EPOCHS\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m), desc\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtraining epoch...\u001b[39m\u001b[39m'\u001b[39m, leave\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B91.243.117.249/work2/home/ing1/corentin/hatespeech-detection-models/src/model/roberta-bce.ipynb#ch0000070vscode-remote?line=2'>3</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m progress:\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B91.243.117.249/work2/home/ing1/corentin/hatespeech-detection-models/src/model/roberta-bce.ipynb#ch0000070vscode-remote?line=3'>4</a>\u001b[0m     \u001b[39m# Train\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B91.243.117.249/work2/home/ing1/corentin/hatespeech-detection-models/src/model/roberta-bce.ipynb#ch0000070vscode-remote?line=4'>5</a>\u001b[0m     train_epoch(epoch_id\u001b[39m=\u001b[39;49mepoch)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B91.243.117.249/work2/home/ing1/corentin/hatespeech-detection-models/src/model/roberta-bce.ipynb#ch0000070vscode-remote?line=6'>7</a>\u001b[0m     \u001b[39m# Validation\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B91.243.117.249/work2/home/ing1/corentin/hatespeech-detection-models/src/model/roberta-bce.ipynb#ch0000070vscode-remote?line=7'>8</a>\u001b[0m     valid_metrics_dict \u001b[39m=\u001b[39m valid_epoch(epoch_id\u001b[39m=\u001b[39mepoch)\n",
      "\u001b[1;32m/work2/home/ing1/corentin/hatespeech-detection-models/src/model/roberta-bce.ipynb Cell 34'\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(epoch_id)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B91.243.117.249/work2/home/ing1/corentin/hatespeech-detection-models/src/model/roberta-bce.ipynb#ch0000057vscode-remote?line=2'>3</a>\u001b[0m logger\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mSTART EPOCH \u001b[39m\u001b[39m{\u001b[39;00mepoch_id\u001b[39m=}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B91.243.117.249/work2/home/ing1/corentin/hatespeech-detection-models/src/model/roberta-bce.ipynb#ch0000057vscode-remote?line=4'>5</a>\u001b[0m progress \u001b[39m=\u001b[39m tqdm(train_dataloader, desc\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtraining batch...\u001b[39m\u001b[39m'\u001b[39m, leave\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B91.243.117.249/work2/home/ing1/corentin/hatespeech-detection-models/src/model/roberta-bce.ipynb#ch0000057vscode-remote?line=5'>6</a>\u001b[0m \u001b[39mfor\u001b[39;00m batch_id, batch \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(progress):\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B91.243.117.249/work2/home/ing1/corentin/hatespeech-detection-models/src/model/roberta-bce.ipynb#ch0000057vscode-remote?line=6'>7</a>\u001b[0m     logger\u001b[39m.\u001b[39mtrace(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mbatch_id\u001b[39m=}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B91.243.117.249/work2/home/ing1/corentin/hatespeech-detection-models/src/model/roberta-bce.ipynb#ch0000057vscode-remote?line=7'>8</a>\u001b[0m     token_list_batch \u001b[39m=\u001b[39m batch[\u001b[39m\"\u001b[39m\u001b[39mids\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/tqdm/notebook.py:258\u001b[0m, in \u001b[0;36mtqdm_notebook.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/tqdm/notebook.py?line=255'>256</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/tqdm/notebook.py?line=256'>257</a>\u001b[0m     it \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39m(tqdm_notebook, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__iter__\u001b[39m()\n\u001b[0;32m--> <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/tqdm/notebook.py?line=257'>258</a>\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m it:\n\u001b[1;32m    <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/tqdm/notebook.py?line=258'>259</a>\u001b[0m         \u001b[39m# return super(tqdm...) will not catch exception\u001b[39;00m\n\u001b[1;32m    <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/tqdm/notebook.py?line=259'>260</a>\u001b[0m         \u001b[39myield\u001b[39;00m obj\n\u001b[1;32m    <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/tqdm/notebook.py?line=260'>261</a>\u001b[0m \u001b[39m# NB: except ... [ as ...] breaks IPython async KeyboardInterrupt\u001b[39;00m\n",
      "File \u001b[0;32m~/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/tqdm/std.py:1195\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/tqdm/std.py?line=1191'>1192</a>\u001b[0m time \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_time\n\u001b[1;32m   <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/tqdm/std.py?line=1193'>1194</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/tqdm/std.py?line=1194'>1195</a>\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m iterable:\n\u001b[1;32m   <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/tqdm/std.py?line=1195'>1196</a>\u001b[0m         \u001b[39myield\u001b[39;00m obj\n\u001b[1;32m   <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/tqdm/std.py?line=1196'>1197</a>\u001b[0m         \u001b[39m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/tqdm/std.py?line=1197'>1198</a>\u001b[0m         \u001b[39m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/torch/utils/data/dataloader.py:530\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/torch/utils/data/dataloader.py?line=527'>528</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/torch/utils/data/dataloader.py?line=528'>529</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()\n\u001b[0;32m--> <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/torch/utils/data/dataloader.py?line=529'>530</a>\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/torch/utils/data/dataloader.py?line=530'>531</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/torch/utils/data/dataloader.py?line=531'>532</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/torch/utils/data/dataloader.py?line=532'>533</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/torch/utils/data/dataloader.py?line=533'>534</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/torch/utils/data/dataloader.py:570\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/torch/utils/data/dataloader.py?line=567'>568</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/torch/utils/data/dataloader.py?line=568'>569</a>\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/torch/utils/data/dataloader.py?line=569'>570</a>\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/torch/utils/data/dataloader.py?line=570'>571</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/torch/utils/data/dataloader.py?line=571'>572</a>\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data)\n",
      "File \u001b[0;32m~/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py?line=46'>47</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfetch\u001b[39m(\u001b[39mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py?line=47'>48</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[0;32m---> <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py?line=48'>49</a>\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py?line=49'>50</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py?line=50'>51</a>\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py?line=46'>47</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfetch\u001b[39m(\u001b[39mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py?line=47'>48</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[0;32m---> <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py?line=48'>49</a>\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py?line=49'>50</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py?line=50'>51</a>\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "\u001b[1;32m/work2/home/ing1/corentin/hatespeech-detection-models/src/model/roberta-bce.ipynb Cell 15'\u001b[0m in \u001b[0;36mJigsawDataset.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B91.243.117.249/work2/home/ing1/corentin/hatespeech-detection-models/src/model/roberta-bce.ipynb#ch0000035vscode-remote?line=9'>10</a>\u001b[0m comment \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39miloc[index][\u001b[39m\"\u001b[39m\u001b[39mcomment_text\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B91.243.117.249/work2/home/ing1/corentin/hatespeech-detection-models/src/model/roberta-bce.ipynb#ch0000035vscode-remote?line=10'>11</a>\u001b[0m label \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39miloc[index][LABEL_LIST]\u001b[39m.\u001b[39mtolist(), dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mfloat)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B91.243.117.249/work2/home/ing1/corentin/hatespeech-detection-models/src/model/roberta-bce.ipynb#ch0000035vscode-remote?line=12'>13</a>\u001b[0m token_list, attention_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtext_to_token_and_mask(comment)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B91.243.117.249/work2/home/ing1/corentin/hatespeech-detection-models/src/model/roberta-bce.ipynb#ch0000035vscode-remote?line=14'>15</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mdict\u001b[39m(index\u001b[39m=\u001b[39mindex, ids\u001b[39m=\u001b[39mtoken_list, mask\u001b[39m=\u001b[39mattention_mask, labels\u001b[39m=\u001b[39mlabel)\n",
      "\u001b[1;32m/work2/home/ing1/corentin/hatespeech-detection-models/src/model/roberta-bce.ipynb Cell 15'\u001b[0m in \u001b[0;36mJigsawDataset.text_to_token_and_mask\u001b[0;34m(self, input_text)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B91.243.117.249/work2/home/ing1/corentin/hatespeech-detection-models/src/model/roberta-bce.ipynb#ch0000035vscode-remote?line=16'>17</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtext_to_token_and_mask\u001b[39m(\u001b[39mself\u001b[39m, input_text):\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B91.243.117.249/work2/home/ing1/corentin/hatespeech-detection-models/src/model/roberta-bce.ipynb#ch0000035vscode-remote?line=17'>18</a>\u001b[0m     tokenization_dict \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39;49mencode_plus(input_text,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B91.243.117.249/work2/home/ing1/corentin/hatespeech-detection-models/src/model/roberta-bce.ipynb#ch0000035vscode-remote?line=18'>19</a>\u001b[0m                             add_special_tokens\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B91.243.117.249/work2/home/ing1/corentin/hatespeech-detection-models/src/model/roberta-bce.ipynb#ch0000035vscode-remote?line=19'>20</a>\u001b[0m                             max_length\u001b[39m=\u001b[39;49m\u001b[39m128\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B91.243.117.249/work2/home/ing1/corentin/hatespeech-detection-models/src/model/roberta-bce.ipynb#ch0000035vscode-remote?line=20'>21</a>\u001b[0m                             padding\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mmax_length\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B91.243.117.249/work2/home/ing1/corentin/hatespeech-detection-models/src/model/roberta-bce.ipynb#ch0000035vscode-remote?line=21'>22</a>\u001b[0m                             truncation\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B91.243.117.249/work2/home/ing1/corentin/hatespeech-detection-models/src/model/roberta-bce.ipynb#ch0000035vscode-remote?line=22'>23</a>\u001b[0m                             return_attention_mask\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B91.243.117.249/work2/home/ing1/corentin/hatespeech-detection-models/src/model/roberta-bce.ipynb#ch0000035vscode-remote?line=23'>24</a>\u001b[0m                             return_tensors\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mpt\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B91.243.117.249/work2/home/ing1/corentin/hatespeech-detection-models/src/model/roberta-bce.ipynb#ch0000035vscode-remote?line=24'>25</a>\u001b[0m     token_list \u001b[39m=\u001b[39m tokenization_dict[\u001b[39m\"\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mflatten()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B91.243.117.249/work2/home/ing1/corentin/hatespeech-detection-models/src/model/roberta-bce.ipynb#ch0000035vscode-remote?line=25'>26</a>\u001b[0m     attention_mask \u001b[39m=\u001b[39m tokenization_dict[\u001b[39m\"\u001b[39m\u001b[39mattention_mask\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mflatten()\n",
      "File \u001b[0;32m~/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2556\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2545'>2546</a>\u001b[0m \u001b[39m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[1;32m   <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2546'>2547</a>\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[1;32m   <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2547'>2548</a>\u001b[0m     padding\u001b[39m=\u001b[39mpadding,\n\u001b[1;32m   <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2548'>2549</a>\u001b[0m     truncation\u001b[39m=\u001b[39mtruncation,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2552'>2553</a>\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m   <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2553'>2554</a>\u001b[0m )\n\u001b[0;32m-> <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2555'>2556</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_encode_plus(\n\u001b[1;32m   <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2556'>2557</a>\u001b[0m     text\u001b[39m=\u001b[39;49mtext,\n\u001b[1;32m   <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2557'>2558</a>\u001b[0m     text_pair\u001b[39m=\u001b[39;49mtext_pair,\n\u001b[1;32m   <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2558'>2559</a>\u001b[0m     add_special_tokens\u001b[39m=\u001b[39;49madd_special_tokens,\n\u001b[1;32m   <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2559'>2560</a>\u001b[0m     padding_strategy\u001b[39m=\u001b[39;49mpadding_strategy,\n\u001b[1;32m   <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2560'>2561</a>\u001b[0m     truncation_strategy\u001b[39m=\u001b[39;49mtruncation_strategy,\n\u001b[1;32m   <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2561'>2562</a>\u001b[0m     max_length\u001b[39m=\u001b[39;49mmax_length,\n\u001b[1;32m   <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2562'>2563</a>\u001b[0m     stride\u001b[39m=\u001b[39;49mstride,\n\u001b[1;32m   <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2563'>2564</a>\u001b[0m     is_split_into_words\u001b[39m=\u001b[39;49mis_split_into_words,\n\u001b[1;32m   <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2564'>2565</a>\u001b[0m     pad_to_multiple_of\u001b[39m=\u001b[39;49mpad_to_multiple_of,\n\u001b[1;32m   <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2565'>2566</a>\u001b[0m     return_tensors\u001b[39m=\u001b[39;49mreturn_tensors,\n\u001b[1;32m   <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2566'>2567</a>\u001b[0m     return_token_type_ids\u001b[39m=\u001b[39;49mreturn_token_type_ids,\n\u001b[1;32m   <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2567'>2568</a>\u001b[0m     return_attention_mask\u001b[39m=\u001b[39;49mreturn_attention_mask,\n\u001b[1;32m   <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2568'>2569</a>\u001b[0m     return_overflowing_tokens\u001b[39m=\u001b[39;49mreturn_overflowing_tokens,\n\u001b[1;32m   <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2569'>2570</a>\u001b[0m     return_special_tokens_mask\u001b[39m=\u001b[39;49mreturn_special_tokens_mask,\n\u001b[1;32m   <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2570'>2571</a>\u001b[0m     return_offsets_mapping\u001b[39m=\u001b[39;49mreturn_offsets_mapping,\n\u001b[1;32m   <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2571'>2572</a>\u001b[0m     return_length\u001b[39m=\u001b[39;49mreturn_length,\n\u001b[1;32m   <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2572'>2573</a>\u001b[0m     verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m   <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2573'>2574</a>\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m   <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2574'>2575</a>\u001b[0m )\n",
      "File \u001b[0;32m~/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/transformers/tokenization_utils.py:647\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/transformers/tokenization_utils.py?line=637'>638</a>\u001b[0m \u001b[39mif\u001b[39;00m return_offsets_mapping:\n\u001b[1;32m    <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/transformers/tokenization_utils.py?line=638'>639</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m(\n\u001b[1;32m    <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/transformers/tokenization_utils.py?line=639'>640</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mreturn_offset_mapping is not available when using Python tokenizers. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/transformers/tokenization_utils.py?line=640'>641</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mTo use this feature, change your tokenizer to one deriving from \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/transformers/tokenization_utils.py?line=643'>644</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mhttps://github.com/huggingface/transformers/pull/2674\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/transformers/tokenization_utils.py?line=644'>645</a>\u001b[0m     )\n\u001b[0;32m--> <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/transformers/tokenization_utils.py?line=646'>647</a>\u001b[0m first_ids \u001b[39m=\u001b[39m get_input_ids(text)\n\u001b[1;32m    <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/transformers/tokenization_utils.py?line=647'>648</a>\u001b[0m second_ids \u001b[39m=\u001b[39m get_input_ids(text_pair) \u001b[39mif\u001b[39;00m text_pair \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/transformers/tokenization_utils.py?line=649'>650</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprepare_for_model(\n\u001b[1;32m    <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/transformers/tokenization_utils.py?line=650'>651</a>\u001b[0m     first_ids,\n\u001b[1;32m    <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/transformers/tokenization_utils.py?line=651'>652</a>\u001b[0m     pair_ids\u001b[39m=\u001b[39msecond_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/transformers/tokenization_utils.py?line=665'>666</a>\u001b[0m     verbose\u001b[39m=\u001b[39mverbose,\n\u001b[1;32m    <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/transformers/tokenization_utils.py?line=666'>667</a>\u001b[0m )\n",
      "File \u001b[0;32m~/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/transformers/tokenization_utils.py:634\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._encode_plus.<locals>.get_input_ids\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m    <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/transformers/tokenization_utils.py?line=629'>630</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/transformers/tokenization_utils.py?line=630'>631</a>\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mInput \u001b[39m\u001b[39m{\u001b[39;00mtext\u001b[39m}\u001b[39;00m\u001b[39m is not valid. Should be a string or a list/tuple of strings when `is_split_into_words=True`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/transformers/tokenization_utils.py?line=631'>632</a>\u001b[0m     )\n\u001b[1;32m    <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/transformers/tokenization_utils.py?line=632'>633</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/transformers/tokenization_utils.py?line=633'>634</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/transformers/tokenization_utils.py?line=634'>635</a>\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mInput \u001b[39m\u001b[39m{\u001b[39;00mtext\u001b[39m}\u001b[39;00m\u001b[39m is not valid. Should be a string, a list/tuple of strings or a list/tuple of integers.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///work2/home/ing1/corentin/hatespeech-detection-models/venv/lib/python3.8/site-packages/transformers/tokenization_utils.py?line=635'>636</a>\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Input nan is not valid. Should be a string, a list/tuple of strings or a list/tuple of integers."
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "progress =  tqdm(range(1,NUM_EPOCHS+1), desc='training epoch...', leave=True)\n",
    "for epoch in progress:\n",
    "    # Train\n",
    "    train_epoch(epoch_id=epoch)\n",
    "\n",
    "    # Validation\n",
    "    valid_metrics_dict = valid_epoch(epoch_id=epoch)\n",
    "\n",
    "    # Save\n",
    "    torch.save(model, MODEL_FILE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>t</th>\n",
       "      <th>a</th>\n",
       "      <th>c</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     t    a    c\n",
       "4  0.5  0.2  0.3\n",
       "6  1.0  0.0  1.0"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame([[1,0,1],[0.5,0.2,0.3]], columns=[\"t\", \"a\", \"c\"], index=[6,4]).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-24 23:07:15.273 | WARNING  | __main__:<cell line: 1>:5 - Train DataFrame & Validation DataFrame already deleted\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    del train_df\n",
    "    del validation_df\n",
    "except NameError:\n",
    "    logger.warning(\"Train DataFrame & Validation DataFrame already deleted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>split</th>\n",
       "      <th>created_date</th>\n",
       "      <th>publication_id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>article_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>funny</th>\n",
       "      <th>wow</th>\n",
       "      <th>...</th>\n",
       "      <th>white</th>\n",
       "      <th>asian</th>\n",
       "      <th>latino</th>\n",
       "      <th>other_race_or_ethnicity</th>\n",
       "      <th>physical_disability</th>\n",
       "      <th>intellectual_or_learning_disability</th>\n",
       "      <th>psychiatric_or_mental_illness</th>\n",
       "      <th>other_disability</th>\n",
       "      <th>identity_annotator_count</th>\n",
       "      <th>toxicity_annotator_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7084460</td>\n",
       "      <td>arresting man resisting arrest cop suckers see...</td>\n",
       "      <td>test</td>\n",
       "      <td>2016-11-01 16:53:33.561631+00</td>\n",
       "      <td>13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>149218</td>\n",
       "      <td>approved</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>7141509</td>\n",
       "      <td>alternative facts go check people like idea ta...</td>\n",
       "      <td>test</td>\n",
       "      <td>2017-01-30 02:53:48.012277+00</td>\n",
       "      <td>21</td>\n",
       "      <td>919529.0</td>\n",
       "      <td>164687</td>\n",
       "      <td>approved</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>7077814</td>\n",
       "      <td>whine sore loser artster enjoy agony</td>\n",
       "      <td>test</td>\n",
       "      <td>2016-12-03 00:17:42.300700+00</td>\n",
       "      <td>54</td>\n",
       "      <td>649753.0</td>\n",
       "      <td>154126</td>\n",
       "      <td>approved</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>7147990</td>\n",
       "      <td>rarely opportunity agree bennet much case righ...</td>\n",
       "      <td>test</td>\n",
       "      <td>2017-09-13 16:37:16.990602+00</td>\n",
       "      <td>102</td>\n",
       "      <td>NaN</td>\n",
       "      <td>377304</td>\n",
       "      <td>approved</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>7008066</td>\n",
       "      <td>law every freedom asss</td>\n",
       "      <td>test</td>\n",
       "      <td>2017-07-09 07:03:44.153492+00</td>\n",
       "      <td>54</td>\n",
       "      <td>5556167.0</td>\n",
       "      <td>353158</td>\n",
       "      <td>approved</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 46 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                       comment_text split  \\\n",
       "3   7084460  arresting man resisting arrest cop suckers see...  test   \n",
       "10  7141509  alternative facts go check people like idea ta...  test   \n",
       "11  7077814               whine sore loser artster enjoy agony  test   \n",
       "38  7147990  rarely opportunity agree bennet much case righ...  test   \n",
       "42  7008066                             law every freedom asss  test   \n",
       "\n",
       "                     created_date  publication_id  parent_id  article_id  \\\n",
       "3   2016-11-01 16:53:33.561631+00              13        NaN      149218   \n",
       "10  2017-01-30 02:53:48.012277+00              21   919529.0      164687   \n",
       "11  2016-12-03 00:17:42.300700+00              54   649753.0      154126   \n",
       "38  2017-09-13 16:37:16.990602+00             102        NaN      377304   \n",
       "42  2017-07-09 07:03:44.153492+00              54  5556167.0      353158   \n",
       "\n",
       "      rating  funny  wow  ...  white  asian  latino  other_race_or_ethnicity  \\\n",
       "3   approved      0    0  ...    NaN    NaN     NaN                      NaN   \n",
       "10  approved      1    0  ...    NaN    NaN     NaN                      NaN   \n",
       "11  approved      0    0  ...    NaN    NaN     NaN                      NaN   \n",
       "38  approved      1    0  ...    NaN    NaN     NaN                      NaN   \n",
       "42  approved      0    0  ...    NaN    NaN     NaN                      NaN   \n",
       "\n",
       "    physical_disability  intellectual_or_learning_disability  \\\n",
       "3                   NaN                                  NaN   \n",
       "10                  NaN                                  NaN   \n",
       "11                  NaN                                  NaN   \n",
       "38                  NaN                                  NaN   \n",
       "42                  NaN                                  NaN   \n",
       "\n",
       "    psychiatric_or_mental_illness  other_disability  identity_annotator_count  \\\n",
       "3                             NaN               NaN                         0   \n",
       "10                            NaN               NaN                         0   \n",
       "11                            NaN               NaN                         0   \n",
       "38                            NaN               NaN                         0   \n",
       "42                            NaN               NaN                         0   \n",
       "\n",
       "    toxicity_annotator_count  \n",
       "3                         76  \n",
       "10                        72  \n",
       "11                        80  \n",
       "38                         9  \n",
       "42                        10  \n",
       "\n",
       "[5 rows x 46 columns]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.read_csv(TEST_DATASET_PATH, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = JigsawDataset(test_df, tokenizer)\n",
    "test_dataloader = DataLoader(test_dataset,\n",
    "                             batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluation(model):\n",
    "    model.eval()\n",
    "    logger.info(f\"START EVALUATION\")\n",
    "\n",
    "    index_tensor = torch.Tensor([])\n",
    "    prediction_tensor = torch.Tensor([])\n",
    "\n",
    "    progress = tqdm(test_dataloader, desc='test batch...', leave=False)\n",
    "    for batch_id, batch in enumerate(progress):\n",
    "        logger.trace(f\"{batch_id=}\")\n",
    "        index_batch = batch[\"index\"].to(device)\n",
    "        token_list_batch = batch[\"ids\"].to(device)\n",
    "        attention_mask_batch = batch[\"mask\"].to(device)\n",
    "        label_batch = batch[\"labels\"].to(device)\n",
    "\n",
    "        # Predict\n",
    "        prediction_batch = model(token_list_batch, attention_mask_batch)\n",
    "        transformed_prediction_batch = prediction_batch.squeeze()\n",
    "        \n",
    "        index_tensor = torch.concat([index_tensor, index_batch.cpu()])\n",
    "        prediction_tensor = torch.concat([prediction_tensor, transformed_prediction_batch.cpu()])\n",
    "    \n",
    "    logger.info(f\"END EVALUATION\")\n",
    "    prediction_test_df = pd.DataFrame(prediction_tensor.tolist(), \n",
    "                                     columns=LABEL_LIST,\n",
    "                                     index=index_tensor.tolist())\n",
    "    prediction_test_df.to_csv(TEST_FILE_PATH)\n",
    "    logger.success(f\"Test predictions exported !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation(model)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "61bdec511af6d11ab513a76379a59f4e5a5400220f7b851eb051cbea952f89aa"
  },
  "kernelspec": {
   "display_name": "Python 3.8.9 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
