{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RoBERTa BCE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables d'environnement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "# Id des GPU disponibles : 0 et 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import datetime\n",
    "from typing import Any, Union, Dict, List\n",
    "import uuid\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchtext\n",
    "import nltk\n",
    "import sklearn\n",
    "import transformers\n",
    "import torchmetrics as tm\n",
    "from torchmetrics import MetricCollection, Metric, Accuracy, Precision, Recall, AUROC, HammingDistance, F1, ROC, AUC, PrecisionRecallCurve\n",
    "\n",
    "\n",
    "from loguru import logger\n",
    "from tqdm.auto import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "# import warnings\n",
    "# warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUSTOME_NAME = \"roberta-bce\"\n",
    "\n",
    "# Dataset\n",
    "DATA_DIR_PATH = os.path.abspath(\"../../data\")\n",
    "TRAIN_DATASET_PATH = os.path.join(DATA_DIR_PATH, \"jigsaw2019-train.csv\")\n",
    "TEST_DATASET_PATH = os.path.join(DATA_DIR_PATH, \"jigsaw2019-test.csv\")\n",
    "LABEL_LIST = ['toxicity', 'severe_toxicity', 'obscene', 'sexual_explicit',\n",
    "            'identity_attack', 'insult', 'threat']\n",
    "IDENTITY_LIST = ['male', 'female', 'transgender', 'other_gender', 'heterosexual',\n",
    "                'homosexual_gay_or_lesbian', 'bisexual','other_sexual_orientation',\n",
    "                'christian', 'jewish', 'muslim', 'hindu','buddhist', 'atheist',\n",
    "                'other_religion', 'black', 'white', 'asian', 'latino',\n",
    "                'other_race_or_ethnicity', 'physical_disability',\n",
    "                'intellectual_or_learning_disability',\n",
    "                'psychiatric_or_mental_illness','other_disability']\n",
    "SELECTED_IDENTITY_LIST = ['male', 'female', 'black', 'white', 'homosexual_gay_or_lesbian',\n",
    "                    'christian', 'jewish', 'muslim', 'psychiatric_or_mental_illness']\n",
    "\n",
    "# Session\n",
    "SESSION_DIR_PATH = os.path.abspath(\"../../session\")\n",
    "SESSION_DATETIME = datetime.datetime.now().strftime(\"%Y-%m-%dT%H-%M-%S-%f\")\n",
    "SESSION_NAME = f\"{CUSTOME_NAME}_{SESSION_DATETIME}\"\n",
    "CURRENT_SESSION_DIR_PATH = os.path.join(SESSION_DIR_PATH, SESSION_NAME)\n",
    "# Créer le dossier de la session\n",
    "os.makedirs(CURRENT_SESSION_DIR_PATH, exist_ok=True)\n",
    "\n",
    "# Architecture de fichier dans `CURRENT_SESSION_DIR_PATH`\n",
    "LOG_FILE_NAME = f\"{SESSION_NAME}.loguru.log\"\n",
    "MODEL_FILE_NAME = f\"{SESSION_NAME}.model\"\n",
    "TEST_FILE_NAME = f\"{SESSION_NAME}.test.csv\"\n",
    "METRIC_FILE_NAME = f\"{SESSION_NAME}.metric.json\"\n",
    "LOG_FILE_PATH = os.path.join(CURRENT_SESSION_DIR_PATH, LOG_FILE_NAME)\n",
    "MODEL_FILE_PATH = os.path.join(CURRENT_SESSION_DIR_PATH, MODEL_FILE_NAME)\n",
    "TEST_FILE_PATH = os.path.join(CURRENT_SESSION_DIR_PATH, TEST_FILE_NAME)\n",
    "METRIC_FILE_PATH = os.path.join(CURRENT_SESSION_DIR_PATH, METRIC_FILE_NAME)\n",
    "\n",
    "# CUDA\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-24 18:36:51.105 | INFO     | __main__:<cell line: 2>:2 - SESSION_NAME='roberta-bce_2022-03-24T18-36-33-957856'\n",
      "2022-03-24 18:36:51.108 | INFO     | __main__:<cell line: 3>:3 - TRAIN_DATASET_PATH='/work2/home/ing1/corentin/hatespeech-detection-models/data/jigsaw2019-train.csv'\n",
      "2022-03-24 18:36:51.110 | INFO     | __main__:<cell line: 4>:4 - TEST_DATASET_PATH='/work2/home/ing1/corentin/hatespeech-detection-models/data/jigsaw2019-test.csv'\n",
      "2022-03-24 18:36:51.112 | INFO     | __main__:<cell line: 5>:5 - CURRENT_SESSION_DIR_PATH='/work2/home/ing1/corentin/hatespeech-detection-models/session/roberta-bce_2022-03-24T18-36-33-957856'\n"
     ]
    }
   ],
   "source": [
    "logger.add(LOG_FILE_PATH, level=\"TRACE\")\n",
    "logger.info(f\"{SESSION_NAME=}\")\n",
    "logger.info(f\"{TRAIN_DATASET_PATH=}\")\n",
    "logger.info(f\"{TEST_DATASET_PATH=}\")\n",
    "logger.info(f\"{CURRENT_SESSION_DIR_PATH=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vérifier la cohérence de l'architecture et l'accès aux ressources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-24 18:34:14.978 | INFO     | __main__:<cell line: 1>:1 - Checking consistency...\n",
      "2022-03-24 18:34:14.983 | SUCCESS  | __main__:<cell line: 10>:10 - Datasets are reachable\n",
      "2022-03-24 18:34:14.986 | INFO     | __main__:<cell line: 15>:15 - GPU_IS_AVAILABLE=True\n",
      "2022-03-24 18:34:14.988 | INFO     | __main__:<cell line: 16>:16 - GPU_COUNT=1\n",
      "2022-03-24 18:34:14.990 | SUCCESS  | __main__:<cell line: 20>:20 - GPU and CUDA are available\n",
      "2022-03-24 18:34:14.992 | INFO     | __main__:<cell line: 21>:21 - device=device(type='cuda')\n",
      "2022-03-24 18:34:14.994 | INFO     | __main__:<cell line: 22>:24 - GPU 0 : NVIDIA TITAN X (Pascal)\n"
     ]
    }
   ],
   "source": [
    "logger.info(f\"Checking consistency...\")\n",
    "\n",
    "# Vérifier l'accès aux datasets\n",
    "if not os.path.exists(TRAIN_DATASET_PATH):\n",
    "    logger.critical(f\"Train dataset does not exist !\")\n",
    "    raise RuntimeError(\"Train dataset does not exist !\")\n",
    "if not os.path.exists(TEST_DATASET_PATH):\n",
    "    logger.critical(f\"Test dataset does not exist !\")\n",
    "    raise RuntimeError(\"Test dataset does not exist !\")\n",
    "logger.success(\"Datasets are reachable\")\n",
    "\n",
    "# Vérifier l'accès aux GPU\n",
    "GPU_IS_AVAILABLE = torch.cuda.is_available()\n",
    "GPU_COUNT = torch.cuda.device_count()\n",
    "logger.info(f\"{GPU_IS_AVAILABLE=}\")\n",
    "logger.info(f\"{GPU_COUNT=}\")\n",
    "if not GPU_IS_AVAILABLE:\n",
    "    logger.critical(\"GPU and CUDA are not available !\")\n",
    "    raise RuntimeError(\"GPU and CUDA are not available !\")\n",
    "logger.success(\"GPU and CUDA are available\")\n",
    "logger.info(f\"{device=}\")\n",
    "for gpu_id in range(GPU_COUNT):\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    logger.info(f\"GPU {gpu_id} : {gpu_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(TRAIN_DATASET_PATH, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remplacer toutes les colonnes correspondantes aux labels par 1 ou 0\n",
    "# si la probabilité est supérieure ou égale à 0.5 ou non\n",
    "train_df[LABEL_LIST] = (train_df[LABEL_LIST]>=0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JigsawDataset(Dataset):\n",
    "    def __init__(self, data_df, tokenizer):\n",
    "        self.data = data_df\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        comment = self.data.iloc[index][\"comment_text\"]\n",
    "        label = torch.tensor(self.data.iloc[index][LABEL_LIST].tolist(), dtype=torch.float)\n",
    "        \n",
    "        token_list, attention_mask = self.text_to_token_and_mask(comment)\n",
    "\n",
    "        return dict(index=index, ids=token_list, mask=attention_mask, labels=label)\n",
    "    \n",
    "    def text_to_token_and_mask(self, input_text):\n",
    "        tokenization_dict = tokenizer.encode_plus(input_text,\n",
    "                                add_special_tokens=True,\n",
    "                                max_length=128,\n",
    "                                padding='max_length',\n",
    "                                truncation=True,\n",
    "                                return_attention_mask=True,\n",
    "                                return_tensors='pt')\n",
    "        token_list = tokenization_dict[\"input_ids\"].flatten()\n",
    "        attention_mask = tokenization_dict[\"attention_mask\"].flatten()\n",
    "        return (token_list, attention_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_lr(optim, lr):\n",
    "    '''\n",
    "    Set the learning rate in the optimizer\n",
    "    '''\n",
    "    for g in optim.param_groups:\n",
    "        g['lr'] = lr\n",
    "    return optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer class and functions for models and predictions\n",
    "\n",
    "class TransformerClassifierStack(nn.Module):\n",
    "    def __init__(self, tr_model, nb_labels, dropout_prob=0.4, freeze=False):\n",
    "        super().__init__()\n",
    "        self.tr_model = tr_model\n",
    "\n",
    "        # Stack features of 4 last encoders\n",
    "        self.hidden_dim = tr_model.config.hidden_size * 4\n",
    "\n",
    "        # hidden linear for the classification\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        self.hl = nn.Linear(self.hidden_dim, self.hidden_dim)\n",
    "\n",
    "        # Last Linear for the classification\n",
    "        self.last_l = nn.Linear(self.hidden_dim, nb_labels)\n",
    "\n",
    "        # freeze all the parameters if necessary\n",
    "        for param in self.tr_model.parameters():\n",
    "            param.requires_grad = not freeze\n",
    "\n",
    "        # init learning params of last layers\n",
    "        torch.nn.init.xavier_uniform_(self.hl.weight)\n",
    "        torch.nn.init.xavier_uniform_(self.last_l.weight)\n",
    "\n",
    "    def forward(self, ids, mask):\n",
    "        # ids = [batch_size, padded_seq_len]\n",
    "        # mask = [batch_size, padded_seq_len]\n",
    "        # mask: avoid to make self attention on padded data\n",
    "        tr_output = self.tr_model(input_ids=ids,\n",
    "                                  attention_mask=mask,\n",
    "                                  output_hidden_states=True)\n",
    "\n",
    "        # Get all the hidden states\n",
    "        hidden_states = tr_output['hidden_states']\n",
    "\n",
    "        # hs_* = [batch_size, padded_seq_len, 768]\n",
    "        hs_1 = hidden_states[-1][:, 0, :]\n",
    "        hs_2 = hidden_states[-2][:, 0, :]\n",
    "        hs_3 = hidden_states[-3][:, 0, :]\n",
    "        hs_4 = hidden_states[-4][:, 0, :]\n",
    "\n",
    "        # features_vec = [batch_size, 768 * 4]\n",
    "        features_vec = torch.cat([hs_1, hs_2, hs_3, hs_4], dim=-1)\n",
    "\n",
    "        x = self.dropout(features_vec)\n",
    "        x = self.hl(x)\n",
    "\n",
    "        # x = [batch_size, 768 * 4]\n",
    "        x = torch.tanh(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.last_l(x)\n",
    "\n",
    "        # x = [batch_size, 1]\n",
    "        return x\n",
    "\n",
    "def load_roberta_model(nb_labels):\n",
    "    '''\n",
    "    Load RoBERTa model without any checkpoint\n",
    "    RoBERTa for finetuning\n",
    "    '''\n",
    "    logger.info(f\"transformers.RobertaTokenizer : roberta-base\")\n",
    "    logger.info(f\"transformers.AutoModel : roberta-base\")\n",
    "    tokenizer = transformers.RobertaTokenizer.from_pretrained('roberta-base')\n",
    "    tr_model = transformers.AutoModel.from_pretrained('roberta-base')\n",
    "    model = TransformerClassifierStack(tr_model, nb_labels)\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "def load_roberta_pretrained(path, nb_labels, lr=2e-5):\n",
    "    '''\n",
    "    Load RoBERTa from checkout point (already trained on Hate Speech tasks)\n",
    "    '''\n",
    "    tokenizer = transformers.RobertaTokenizer.from_pretrained('roberta-base')\n",
    "    tr_model = transformers.AutoModel.from_pretrained('roberta-base')\n",
    "    model = TransformerClassifierStack(tr_model, nb_labels)\n",
    "\n",
    "    loaded = torch.load(path)\n",
    "    model.load_state_dict(loaded['state_dict'])\n",
    "\n",
    "    optimizer = transformers.AdamW(model.parameters(), lr=lr)\n",
    "    optimizer.load_state_dict(loaded['optimizer_dict'])\n",
    "    optimizer = set_lr(optimizer, lr)\n",
    "\n",
    "    return model, tokenizer, optimizer\n",
    "\n",
    "def preds_fn(batch, model, device):\n",
    "    '''\n",
    "    Get the predictions for one batch according to the model\n",
    "    '''\n",
    "    b_input = batch['ids'].to(device)\n",
    "    b_mask = batch['mask'].to(device)\n",
    "\n",
    "    return model(b_input, b_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "model, tokenizer = load_roberta_model(nb_labels=len(LABEL_LIST))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparamètre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "LR=1e-4\n",
    "PIN_MEMORY = True\n",
    "NUM_WORKERS = 0\n",
    "PREFETCH_FACTOR = 2\n",
    "NUM_EPOCHS = 1\n",
    "logger.info(f\"{BATCH_SIZE=}\")\n",
    "logger.info(f\"{LR=}\")\n",
    "logger.info(f\"{PIN_MEMORY=}\")\n",
    "logger.info(f\"{NUM_WORKERS=}\")\n",
    "logger.info(f\"{PREFETCH_FACTOR=}\")\n",
    "logger.info(f\"{NUM_EPOCHS=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = JigsawDataset(train_df, tokenizer)\n",
    "train_dataloader = DataLoader(train_dataset,\n",
    "                             batch_size=BATCH_SIZE,\n",
    "                             shuffle=True,\n",
    "                             num_workers=NUM_WORKERS,\n",
    "                             prefetch_factor=PREFETCH_FACTOR,\n",
    "                             pin_memory=PIN_MEMORY)\n",
    "\n",
    "# Pseudo validation car c'est un sous-ensemble du jeu d'entrainement\n",
    "# C'est juste pour savoir si l'entraînement s'est bien passé\n",
    "# Il ne faut pas s'en servir pour optimiser les hyperparamètres\n",
    "validation_df = train_df.sample(n=50_000)\n",
    "validation_dataset = JigsawDataset(validation_df, tokenizer)\n",
    "validation_dataloader = DataLoader(validation_dataset,\n",
    "                             batch_size=BATCH_SIZE,\n",
    "                             shuffle=True,\n",
    "                             num_workers=NUM_WORKERS,\n",
    "                             prefetch_factor=PREFETCH_FACTOR,\n",
    "                             pin_memory=PIN_MEMORY)\n",
    "\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "logger.info(criterion)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR)\n",
    "logger.info(optimizer)\n",
    "\n",
    "model.to(device)\n",
    "criterion.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variantes de Hamming Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HammingLossWithoutThreshold(Metric):\n",
    "    def __init__(self, num_classes=1, dist_sync_on_step=False):\n",
    "        super().__init__(dist_sync_on_step=dist_sync_on_step)\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        self.add_state(\"total\", default=torch.tensor(0), dist_reduce_fx=\"sum\")\n",
    "        self.add_state(\"nbr_sample\", default=torch.tensor(0), dist_reduce_fx=\"sum\")\n",
    "\n",
    "    def update(self, preds: torch.Tensor, target: torch.Tensor):\n",
    "        current_nbr_sample, current_nbr_category = preds.shape\n",
    "        if current_nbr_category != self.num_classes:\n",
    "          raise AttributeError(\"`num_classes` != `current_nbr_category` detected in `pred` parameter\")\n",
    "        \n",
    "        current_loss_per_pred = torch.absolute(target - preds)\n",
    "        current_hamming_loss = current_loss_per_pred.sum()\n",
    "\n",
    "        self.total += current_hamming_loss\n",
    "        self.nbr_sample += current_nbr_sample\n",
    "\n",
    "    def compute(self):\n",
    "        return self.total/(self.num_classes*self.nbr_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RebalancedHammingLossWithoutThreshold(Metric):\n",
    "    def __init__(self, num_classes=1, average=\"macro\", dist_sync_on_step=False):\n",
    "        super().__init__(dist_sync_on_step=dist_sync_on_step)\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        # average = \"macro\" or None\n",
    "        self.average = average\n",
    "\n",
    "        # Nombre de positif 1 & negatif 0 par categorie\n",
    "        self.add_state(\n",
    "            \"number_positive\",\n",
    "            default=torch.tensor([0 for _ in range(num_classes)]),\n",
    "            dist_reduce_fx=\"sum\",\n",
    "        )\n",
    "        self.add_state(\n",
    "            \"number_negative\",\n",
    "            default=torch.tensor([0 for _ in range(num_classes)]),\n",
    "            dist_reduce_fx=\"sum\",\n",
    "        )\n",
    "\n",
    "        self.add_state(\n",
    "            \"hamming_loss_positive\",\n",
    "            default=torch.tensor([0.0 for _ in range(num_classes)]),\n",
    "            dist_reduce_fx=\"sum\",\n",
    "        )\n",
    "        self.add_state(\n",
    "            \"hamming_loss_negative\",\n",
    "            default=torch.tensor([0.0 for _ in range(num_classes)]),\n",
    "            dist_reduce_fx=\"sum\",\n",
    "        )\n",
    "\n",
    "        self.add_state(\"nbr_sample\", default=torch.tensor(0), dist_reduce_fx=\"sum\")\n",
    "\n",
    "    def update(self, preds: torch.Tensor, target: torch.Tensor):\n",
    "        current_nbr_sample, current_nbr_category = preds.shape\n",
    "        if current_nbr_category != self.num_classes:\n",
    "            raise AttributeError(\n",
    "                \"`num_classes` != `current_nbr_category` detected in `pred` parameter\"\n",
    "            )\n",
    "\n",
    "        # Nombre de positif 1 & negatif 0 par categorie\n",
    "        current_number_positive = target.sum(axis=0)\n",
    "        current_number_negative = current_nbr_sample - target.sum(axis=0)\n",
    "\n",
    "        self.number_positive += current_number_positive.int()\n",
    "        self.number_negative += current_number_negative.int()\n",
    "\n",
    "        self.nbr_sample += current_nbr_sample\n",
    "\n",
    "        for class_id in range(self.num_classes):\n",
    "            positive_filter = target[:, class_id] == 1\n",
    "            negative_filter = target[:, class_id] == 0\n",
    "\n",
    "            target_vector = target[:, class_id]\n",
    "            preds_vector = preds[:, class_id]\n",
    "\n",
    "            # Filtered vector\n",
    "            ## Target\n",
    "            pos_filtered_target_vector = target_vector[positive_filter]\n",
    "            neg_filtered_target_vector = target_vector[negative_filter]\n",
    "            ## Preds\n",
    "            pos_filtered_preds_vector = preds_vector[positive_filter]\n",
    "            neg_filtered_preds_vector = preds_vector[negative_filter]\n",
    "\n",
    "            # Hamming Loss without Threshold\n",
    "            hamming_loss_on_positive = torch.absolute(\n",
    "                pos_filtered_target_vector - pos_filtered_preds_vector\n",
    "            )\n",
    "            hamming_loss_on_negative = torch.absolute(\n",
    "                neg_filtered_target_vector - neg_filtered_preds_vector\n",
    "            )\n",
    "\n",
    "            self.hamming_loss_positive[class_id] += hamming_loss_on_positive.sum()\n",
    "            self.hamming_loss_negative[class_id] += hamming_loss_on_negative.sum()\n",
    "\n",
    "    def compute(self):\n",
    "        factor_pos = self.nbr_sample / (2 * self.number_positive)\n",
    "        factor_neg = self.nbr_sample / (2 * self.number_negative)\n",
    "\n",
    "        rebalanced_hamming_loss_per_class = torch.multiply(\n",
    "            self.hamming_loss_positive, factor_pos\n",
    "        ) + torch.multiply(self.hamming_loss_negative, factor_neg)\n",
    "        if self.average == \"macro\":\n",
    "            return rebalanced_hamming_loss_per_class.sum() / (\n",
    "                self.nbr_sample * self.num_classes\n",
    "            )\n",
    "        return rebalanced_hamming_loss_per_class / (self.nbr_sample)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instanciation des metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(LABEL_LIST)\n",
    "train_metric_dict = dict()\n",
    "\n",
    "# AUROC Macro\n",
    "auroc_macro = AUROC(num_classes=num_classes, compute_on_step=True, average=\"macro\")\n",
    "train_metric_dict[\"auroc_macro\"] = auroc_macro\n",
    "\n",
    "# AUROC per class\n",
    "auroc_per_class = AUROC(num_classes=num_classes, compute_on_step=True, average=None)\n",
    "train_metric_dict[\"auroc_per_class\"] = auroc_per_class\n",
    "\n",
    "# Hamming Distance without Threshold\n",
    "hamming_loss_woutt = HammingLossWithoutThreshold(num_classes=num_classes)\n",
    "train_metric_dict[\"hamming_loss_without_threshold\"] = hamming_loss_woutt\n",
    "\n",
    "# Rebalanced Hamming Distance without Threshold macro\n",
    "rebalanced_hamming_loss_woutt_macro = RebalancedHammingLossWithoutThreshold(\n",
    "    num_classes=num_classes, average=\"macro\"\n",
    ")\n",
    "train_metric_dict[\n",
    "    \"rebalanced_hamming_loss_without_threshold_macro\"\n",
    "] = rebalanced_hamming_loss_woutt_macro\n",
    "\n",
    "# Rebalanced Hamming Distance without Threshold macro\n",
    "rebalanced_hamming_loss_woutt_per_class = RebalancedHammingLossWithoutThreshold(\n",
    "    num_classes=num_classes, average=None\n",
    ")\n",
    "train_metric_dict[\n",
    "    \"rebalanced_hamming_loss_without_threshold_per_class\"\n",
    "] = rebalanced_hamming_loss_woutt_per_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_metric = MetricCollection(train_metric_dict)\n",
    "train_metric.to(device)\n",
    "\n",
    "validation_metric = train_metric.clone()\n",
    "validation_metric.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def serialize(object_to_serialize: Any, ensure_ascii: bool = True) -> str:\n",
    "    \"\"\"\n",
    "    Serialize any object, i.e. convert an object to JSON\n",
    "    Args:\n",
    "        object_to_serialize (Any): The object to serialize\n",
    "        ensure_ascii (bool, optional): If ensure_ascii is true (the default), the output is guaranteed to have all incoming non-ASCII characters escaped. If ensure_ascii is false, these characters will be output as-is. Defaults to True.\n",
    "    Returns:\n",
    "            str: string of serialized object (JSON)\n",
    "    \"\"\"\n",
    "\n",
    "    def dumper(obj: Any) -> Union[str, Dict]:\n",
    "        \"\"\"\n",
    "        Function called recursively by json.dumps to know how to serialize an object.\n",
    "        For example, for datetime, we try to convert it to ISO format rather than\n",
    "        retrieve the list of attributes defined in its object.\n",
    "        Args:\n",
    "            obj (Any): The object to serialize\n",
    "        Returns:\n",
    "            Union[str, Dict]: Serialized object\n",
    "        \"\"\"\n",
    "        if isinstance(obj, torch.Tensor):\n",
    "            return obj.cpu().numpy().tolist()\n",
    "        elif hasattr(obj, \"__dict__\"):\n",
    "            return obj.__dict__\n",
    "        return str(obj)\n",
    "\n",
    "    return json.dumps(object_to_serialize, default=dumper, ensure_ascii=ensure_ascii)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_metric(metric_collection, **kwargs):\n",
    "    \"\"\"\n",
    "    Export MetricCollection to json file\n",
    "\n",
    "    Args:\n",
    "        metric_collection: MetricCollection\n",
    "        **kwargs: field to add in json line\n",
    "    \"\"\"\n",
    "    with open(METRIC_FILE_PATH, \"a\") as f:\n",
    "        metric_collection_value = metric_collection.compute()\n",
    "        metric_collection_value.update(kwargs)\n",
    "        serialized_value = serialize(metric_collection_value)\n",
    "        f.write(serialized_value)\n",
    "        f.write(\"\\n\")\n",
    "    logger.success(\"Metrics are exported !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entraînement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(epoch_id=None):\n",
    "    model.train()\n",
    "    logger.info(f\"START EPOCH {epoch_id=}\")\n",
    "\n",
    "    progress = tqdm(train_dataloader, desc='training batch...', leave=False)\n",
    "    for batch_id, batch in enumerate(progress):\n",
    "        logger.trace(f\"{batch_id=}\")\n",
    "        token_list_batch = batch[\"ids\"].to(device)\n",
    "        attention_mask_batch = batch[\"mask\"].to(device)\n",
    "        label_batch = batch[\"labels\"].to(device)\n",
    "\n",
    "        # Predict\n",
    "        prediction_batch = model(token_list_batch, attention_mask_batch)\n",
    "        transformed_prediction_batch = prediction_batch.squeeze()\n",
    "\n",
    "        # Loss\n",
    "        loss = criterion(transformed_prediction_batch.to(torch.float32), label_batch.to(torch.float32))\n",
    "\n",
    "        # Metrics\n",
    "        train_metrics_collection_dict = train_metric(transformed_prediction_batch, label_batch)\n",
    "        logger.trace(train_metrics_collection_dict)\n",
    "\n",
    "        # Backprop\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update progress bar description\n",
    "        progress_description = \"Train Loss : {loss:.4f} - Train AUROC : {acc:.4f}\"\n",
    "        auroc_macro_value = float(train_metrics_collection_dict[\"auroc_macro\"])\n",
    "        progress_description = progress_description.format(loss=loss.item(), acc=auroc_macro_value)\n",
    "        progress.set_description(progress_description)\n",
    "\n",
    "    logger.info(f\"END EPOCH {epoch_id=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def valid_epoch(epoch_id=None):\n",
    "    model.eval()\n",
    "    logger.info(f\"START VALIDATION {epoch_id=}\")\n",
    "    validation_metric.reset()\n",
    "\n",
    "    loss_list = []\n",
    "    prediction_list = torch.Tensor([])\n",
    "    target_list = torch.Tensor([])\n",
    "\n",
    "\n",
    "    progress = tqdm(validation_dataloader, desc=\"valid batch...\", leave=False)\n",
    "    for batch_id, batch in enumerate(progress):\n",
    "        logger.trace(f\"{batch_id=}\")\n",
    "        \n",
    "        token_list_batch = batch[\"ids\"].to(device)\n",
    "        attention_mask_batch = batch[\"mask\"].to(device)\n",
    "        label_batch = batch[\"labels\"].to(device)\n",
    "\n",
    "        # Predict\n",
    "        prediction_batch = model(token_list_batch, attention_mask_batch)\n",
    "\n",
    "        transformed_prediction_batch = prediction_batch.squeeze()\n",
    "\n",
    "        # Loss\n",
    "        loss = criterion(\n",
    "            transformed_prediction_batch.to(torch.float32),\n",
    "            label_batch.to(torch.float32),\n",
    "        )\n",
    "        loss_list.append(loss.item())\n",
    "        prediction_list = torch.concat(\n",
    "            [prediction_list, transformed_prediction_batch.cpu()]\n",
    "        )\n",
    "        target_list = torch.concat([target_list, label_batch.cpu()])\n",
    "\n",
    "        # Metrics\n",
    "        validation_metric(transformed_prediction_batch, label_batch)\n",
    "\n",
    "    loss_mean = np.mean(loss_list)\n",
    "    logger.trace(validation_metric.compute())\n",
    "    logger.info(f\"END VALIDATION {epoch_id=}\")\n",
    "    export_metric(validation_metric, epoch_id=epoch_id, loss=loss_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "progress =  tqdm(range(1,NUM_EPOCHS+1), desc='training epoch...', leave=True)\n",
    "for epoch in progress:\n",
    "    # Train\n",
    "    train_epoch(epoch_id=epoch)\n",
    "\n",
    "    # Validation\n",
    "    valid_metrics_dict = valid_epoch(epoch_id=epoch)\n",
    "\n",
    "    # Save\n",
    "    torch.save(model, MODEL_FILE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>t</th>\n",
       "      <th>a</th>\n",
       "      <th>c</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     t    a    c\n",
       "4  0.5  0.2  0.3\n",
       "6  1.0  0.0  1.0"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame([[1,0,1],[0.5,0.2,0.3]], columns=[\"t\", \"a\", \"c\"], index=[6,4]).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-24 23:07:15.273 | WARNING  | __main__:<cell line: 1>:5 - Train DataFrame & Validation DataFrame already deleted\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    del train_df\n",
    "    del validation_df\n",
    "except NameError:\n",
    "    logger.warning(\"Train DataFrame & Validation DataFrame already deleted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>split</th>\n",
       "      <th>created_date</th>\n",
       "      <th>publication_id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>article_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>funny</th>\n",
       "      <th>wow</th>\n",
       "      <th>...</th>\n",
       "      <th>white</th>\n",
       "      <th>asian</th>\n",
       "      <th>latino</th>\n",
       "      <th>other_race_or_ethnicity</th>\n",
       "      <th>physical_disability</th>\n",
       "      <th>intellectual_or_learning_disability</th>\n",
       "      <th>psychiatric_or_mental_illness</th>\n",
       "      <th>other_disability</th>\n",
       "      <th>identity_annotator_count</th>\n",
       "      <th>toxicity_annotator_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7084460</td>\n",
       "      <td>arresting man resisting arrest cop suckers see...</td>\n",
       "      <td>test</td>\n",
       "      <td>2016-11-01 16:53:33.561631+00</td>\n",
       "      <td>13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>149218</td>\n",
       "      <td>approved</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>7141509</td>\n",
       "      <td>alternative facts go check people like idea ta...</td>\n",
       "      <td>test</td>\n",
       "      <td>2017-01-30 02:53:48.012277+00</td>\n",
       "      <td>21</td>\n",
       "      <td>919529.0</td>\n",
       "      <td>164687</td>\n",
       "      <td>approved</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>7077814</td>\n",
       "      <td>whine sore loser artster enjoy agony</td>\n",
       "      <td>test</td>\n",
       "      <td>2016-12-03 00:17:42.300700+00</td>\n",
       "      <td>54</td>\n",
       "      <td>649753.0</td>\n",
       "      <td>154126</td>\n",
       "      <td>approved</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>7147990</td>\n",
       "      <td>rarely opportunity agree bennet much case righ...</td>\n",
       "      <td>test</td>\n",
       "      <td>2017-09-13 16:37:16.990602+00</td>\n",
       "      <td>102</td>\n",
       "      <td>NaN</td>\n",
       "      <td>377304</td>\n",
       "      <td>approved</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>7008066</td>\n",
       "      <td>law every freedom asss</td>\n",
       "      <td>test</td>\n",
       "      <td>2017-07-09 07:03:44.153492+00</td>\n",
       "      <td>54</td>\n",
       "      <td>5556167.0</td>\n",
       "      <td>353158</td>\n",
       "      <td>approved</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 46 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                       comment_text split  \\\n",
       "3   7084460  arresting man resisting arrest cop suckers see...  test   \n",
       "10  7141509  alternative facts go check people like idea ta...  test   \n",
       "11  7077814               whine sore loser artster enjoy agony  test   \n",
       "38  7147990  rarely opportunity agree bennet much case righ...  test   \n",
       "42  7008066                             law every freedom asss  test   \n",
       "\n",
       "                     created_date  publication_id  parent_id  article_id  \\\n",
       "3   2016-11-01 16:53:33.561631+00              13        NaN      149218   \n",
       "10  2017-01-30 02:53:48.012277+00              21   919529.0      164687   \n",
       "11  2016-12-03 00:17:42.300700+00              54   649753.0      154126   \n",
       "38  2017-09-13 16:37:16.990602+00             102        NaN      377304   \n",
       "42  2017-07-09 07:03:44.153492+00              54  5556167.0      353158   \n",
       "\n",
       "      rating  funny  wow  ...  white  asian  latino  other_race_or_ethnicity  \\\n",
       "3   approved      0    0  ...    NaN    NaN     NaN                      NaN   \n",
       "10  approved      1    0  ...    NaN    NaN     NaN                      NaN   \n",
       "11  approved      0    0  ...    NaN    NaN     NaN                      NaN   \n",
       "38  approved      1    0  ...    NaN    NaN     NaN                      NaN   \n",
       "42  approved      0    0  ...    NaN    NaN     NaN                      NaN   \n",
       "\n",
       "    physical_disability  intellectual_or_learning_disability  \\\n",
       "3                   NaN                                  NaN   \n",
       "10                  NaN                                  NaN   \n",
       "11                  NaN                                  NaN   \n",
       "38                  NaN                                  NaN   \n",
       "42                  NaN                                  NaN   \n",
       "\n",
       "    psychiatric_or_mental_illness  other_disability  identity_annotator_count  \\\n",
       "3                             NaN               NaN                         0   \n",
       "10                            NaN               NaN                         0   \n",
       "11                            NaN               NaN                         0   \n",
       "38                            NaN               NaN                         0   \n",
       "42                            NaN               NaN                         0   \n",
       "\n",
       "    toxicity_annotator_count  \n",
       "3                         76  \n",
       "10                        72  \n",
       "11                        80  \n",
       "38                         9  \n",
       "42                        10  \n",
       "\n",
       "[5 rows x 46 columns]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.read_csv(TEST_DATASET_PATH, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = JigsawDataset(test_df, tokenizer)\n",
    "test_dataloader = DataLoader(test_dataset,\n",
    "                             batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluation(model):\n",
    "    model.eval()\n",
    "    logger.info(f\"START EVALUATION\")\n",
    "\n",
    "    index_tensor = torch.Tensor([])\n",
    "    prediction_tensor = torch.Tensor([])\n",
    "\n",
    "    progress = tqdm(test_dataloader, desc='test batch...', leave=False)\n",
    "    for batch_id, batch in enumerate(progress):\n",
    "        logger.trace(f\"{batch_id=}\")\n",
    "        index_batch = batch[\"index\"].to(device)\n",
    "        token_list_batch = batch[\"ids\"].to(device)\n",
    "        attention_mask_batch = batch[\"mask\"].to(device)\n",
    "        label_batch = batch[\"labels\"].to(device)\n",
    "\n",
    "        # Predict\n",
    "        prediction_batch = model(token_list_batch, attention_mask_batch)\n",
    "        transformed_prediction_batch = prediction_batch.squeeze()\n",
    "        \n",
    "        index_tensor = torch.concat([index_tensor, index_batch.cpu()])\n",
    "        prediction_tensor = torch.concat([prediction_tensor, transformed_prediction_batch.cpu()])\n",
    "    \n",
    "    logger.info(f\"END EVALUATION\")\n",
    "    prediction_test_df = pd.DataFrame(prediction_tensor.tolist(), \n",
    "                                     columns=LABEL_LIST,\n",
    "                                     index=index_tensor.tolist())\n",
    "    prediction_test_df.to_csv(TEST_FILE_PATH)\n",
    "    logger.success(f\"Test predictions exported !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation(model)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "61bdec511af6d11ab513a76379a59f4e5a5400220f7b851eb051cbea952f89aa"
  },
  "kernelspec": {
   "display_name": "Python 3.8.9 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
